window.searchIndex = {"fields":["title","body"],"pipeline":["trimmer","stopWordFilter","stemmer"],"ref":"id","version":"0.9.5","index":{"body":{"root":{"docs":{},"df":0,"0":{"docs":{},"df":0,".":{"docs":{},"df":0,"9":{"docs":{},"df":0,"5":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":1}}}},"1":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":4,"2":{"docs":{},"df":0,".":{"docs":{},"df":0,"2":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0}},"df":1}}},"4":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":1}},"2":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":5,"0":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":1,"b":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":1}},"5":{"docs":{},"df":0,".":{"docs":{},"df":0,"7":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":1}}},"d":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1}},"3":{"docs":{},"df":0,"0":{"docs":{},"df":0,"b":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":1}},"2":{"docs":{},"df":0,".":{"docs":{},"df":0,"8":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1}}},"d":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":3.0}},"df":1}},"4":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.4142135623730951}},"df":2,"v":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":1}},"6":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0}},"df":2,"4":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1},"b":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":1}},"7":{"docs":{},"df":0,"0":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1,"b":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":1}},"5":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":2},"b":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.4142135623730951}},"df":1}},"8":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":1,".":{"docs":{},"df":0,"7":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0}},"df":1}}},"9":{"docs":{},"df":0,"1":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1},"5":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1}},"a":{"docs":{},"df":0,"b":{"docs":{},"df":0,"i":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":2.449489742783178},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":7}},"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.4142135623730951}},"df":1}}},"o":{"docs":{},"df":0,"v":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":3}},"s":{"docs":{},"df":0,"t":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.4142135623730951}},"df":2}}}}}},"u":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":1}}},"c":{"docs":{},"df":0,"c":{"docs":{},"df":0,"u":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.4142135623730951}},"df":3,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":2.449489742783178},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":7}}}}}},"h":{"docs":{},"df":0,"i":{"docs":{},"df":0,"e":{"docs":{},"df":0,"v":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":10}}}},"k":{"docs":{},"df":0,"n":{"docs":{},"df":0,"o":{"docs":{},"df":0,"w":{"docs":{},"df":0,"l":{"docs":{},"df":0,"e":{"docs":{},"df":0,"d":{"docs":{},"df":0,"g":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":3}}}}}}}},"t":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":4,"i":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1}},"v":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":2.6457513110645907},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":2.8284271247461903},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.7320508075688772}},"df":5}},"o":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":2.449489742783178}},"df":1}}}},"d":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":2,"a":{"docs":{},"df":0,"p":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":2.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":2.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.4142135623730951}},"df":3}}},"d":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":3.3166247903554}},"df":4,"i":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":1}}}}},"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"s":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":10}}}}},"j":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":1}},"u":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":2}}}},"o":{"docs":{},"df":0,"p":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.4142135623730951}},"df":1}}},"v":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"c":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":8},"t":{"docs":{},"df":0,"a":{"docs":{},"df":0,"g":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":4}}}}},"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"s":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":2.0}},"df":2}}}}}}}},"f":{"docs":{},"df":0,"f":{"docs":{},"df":0,"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":2}}},"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0}},"df":1}}}}},"g":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":4}}}}},"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1}}},"n":{"docs":{},"df":0,"o":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1}}}}},"i":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":2.23606797749979},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":3.3166247903554},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.4142135623730951}},"df":4},"l":{"docs":{},"df":0,"g":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{},"df":0,"h":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":4.69041575982343}},"df":2}}}}}}},"i":{"docs":{},"df":0,"g":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":1}}},"l":{"docs":{},"df":0,"o":{"docs":{},"df":0,"w":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":6}}},"o":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":1}},"p":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"a":{"docs":{},"df":0,"e":{"docs":{},"df":0,"v":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0}},"df":1}}}}}},"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":3}}},"h":{"docs":{},"df":0,"o":{"docs":{},"df":0,"u":{"docs":{},"df":0,"g":{"docs":{},"df":0,"h":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1}}}}}},"w":{"docs":{},"df":0,"a":{"docs":{},"df":0,"y":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":4}}}},"m":{"docs":{},"df":0,"a":{"docs":{},"df":0,"z":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":3}},"b":{"docs":{},"df":0,"i":{"docs":{},"df":0,"g":{"docs":{},"df":0,"u":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0}},"df":1}}}},"p":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1}},"n":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{},"df":0,"y":{"docs":{},"df":0,"s":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":2.6457513110645907},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":2.23606797749979}},"df":7}},"z":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.4142135623730951}},"df":3}}}},"i":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0}},"df":1}},"o":{"docs":{},"df":0,"t":{"docs":{},"df":0,"h":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0}},"df":1}}},"t":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{},"df":0,"y":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":1}}}}},"y":{"docs":{},"df":0,"t":{"docs":{},"df":0,"h":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":2.8284271247461903}},"df":1}}}},"p":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"t":{"docs":{},"df":0,"u":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.4142135623730951}},"df":1}}}}},"i":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":1},"p":{"docs":{},"df":0,"l":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":7,"c":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.4142135623730951}},"df":8}}},"r":{"docs":{},"df":0,"o":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"h":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":2.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":2.449489742783178},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.7320508075688772}},"df":13}}},"x":{"docs":{},"df":0,"i":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.4142135623730951}},"df":1}}}}}}},"r":{"docs":{},"df":0,"/":{"docs":{},"df":0,"v":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0}},"df":1}}},"c":{"docs":{},"df":0,"h":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{},"df":0,"u":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":2.23606797749979}},"df":7}}}}}}}}},"e":{"docs":{},"df":0,"a":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":4}},"i":{"docs":{},"df":0,"t":{"docs":{},"df":0,"h":{"docs":{},"df":0,"m":{"docs":{},"df":0,"e":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":2}}}}}},"t":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":3,"i":{"docs":{},"df":0,"c":{"docs":{},"df":0,"u":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":3.872983346207417}},"df":1}}}}}},"s":{"docs":{},"df":0,"k":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0}},"df":1},"p":{"docs":{},"df":0,"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":1}}}},"s":{"docs":{},"df":0,"e":{"docs":{},"df":0,"s":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":5}},"t":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951}},"df":1}},"i":{"docs":{},"df":0,"g":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":2.0}},"df":1}},"s":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":2}}},"o":{"docs":{},"df":0,"c":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":1}}},"u":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1,"p":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":2.0}},"df":2}}}}},"t":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":2.23606797749979}},"df":2}},"t":{"docs":{},"df":0,"t":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"k":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":2.449489742783178}},"df":2}}},"e":{"docs":{},"df":0,"m":{"docs":{},"df":0,"p":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":1}}},"n":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":1}}}}},"u":{"docs":{},"df":0,"c":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":1},"t":{"docs":{},"df":0,"h":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":3}}},"o":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"c":{"docs":{},"df":0,"o":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":1}}}}},"m":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":2.23606797749979}},"df":3,"a":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.4142135623730951}},"df":2}}},"n":{"docs":{},"df":0,"o":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.4142135623730951}},"df":3}}}}}},"v":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":2}}},"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"u":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":5}},"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"g":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":2.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.4142135623730951}},"df":5}}}},"o":{"docs":{},"df":0,"i":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":1}}}},"w":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":2.0}},"df":1}}}},"b":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.4142135623730951}},"df":1,"a":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":1},"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"c":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":2.0}},"df":1}}}},"r":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.7320508075688772}},"df":1}}}}},"s":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":2.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":2.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":2.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":9,"l":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":7}}}},"i":{"docs":{},"df":0,"c":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.4142135623730951}},"df":2}}}},"b":{"docs":{},"df":0,"v":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":1}},"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":1}},"c":{"docs":{},"df":0,"o":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1}}},"f":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":2}}},"g":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":1}}},"h":{"docs":{},"df":0,"a":{"docs":{},"df":0,"v":{"docs":{},"df":0,"i":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":3}}}}},"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":2}}}},"l":{"docs":{},"df":0,"i":{"docs":{},"df":0,"e":{"docs":{},"df":0,"f":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":1}}}},"n":{"docs":{},"df":0,"c":{"docs":{},"df":0,"h":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951}},"df":2,"m":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"k":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":2.23606797749979},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":6}}}}}},"e":{"docs":{},"df":0,"f":{"docs":{},"df":0,"i":{"docs":{},"df":0,"c":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":2}},"t":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":2}}}}},"s":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.4142135623730951}},"df":2}},"t":{"docs":{},"df":0,"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":3}}},"w":{"docs":{},"df":0,"e":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":4}}}}},"y":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.7320508075688772}},"df":5}}}}},"i":{"docs":{},"df":0,"a":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":5,"s":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":5}},"g":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0}},"df":1},"r":{"docs":{},"df":0,"d":{"docs":{},"df":0,"'":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1}}},"t":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":3.0}},"df":1}},"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"k":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":2.0}},"df":2}}},"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.4142135623730951}},"df":1}}},"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":1}}},"o":{"docs":{},"df":0,"c":{"docs":{},"df":0,"k":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":2}},"g":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":3},"o":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":1}}}},"o":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1}},"s":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0}},"df":1}}},"o":{"docs":{},"df":0,"l":{"docs":{},"df":0,"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.7320508075688772}},"df":1}}}},"s":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":7}}},"t":{"docs":{},"df":0,"h":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.7320508075688772}},"df":8}},"u":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":2.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":3.872983346207417}},"df":2}}},"x":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":2.0}},"df":2}},"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":1}},"n":{"docs":{},"df":0,"c":{"docs":{},"df":0,"h":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":1}}}},"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"k":{"docs":{},"df":0,"t":{"docs":{},"df":0,"h":{"docs":{},"df":0,"r":{"docs":{},"df":0,"o":{"docs":{},"df":0,"u":{"docs":{},"df":0,"g":{"docs":{},"df":0,"h":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0}},"df":1}}}}}}}}}},"o":{"docs":{},"df":0,"a":{"docs":{},"df":0,"d":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":3}}}}}},"u":{"docs":{},"df":0,"g":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951}},"df":1},"i":{"docs":{},"df":0,"l":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":2},"t":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":1}}}},"y":{"docs":{},"df":0,"p":{"docs":{},"df":0,"a":{"docs":{},"df":0,"s":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":1}}}}}},"c":{"docs":{"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":2.0}},"df":1,"a":{"docs":{},"df":0,"l":{"docs":{},"df":0,"c":{"docs":{},"df":0,"u":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":1}}},"i":{"docs":{},"df":0,"b":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":2.6457513110645907}},"df":1}}},"l":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.4142135623730951}},"df":2}},"m":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":1}},"p":{"docs":{},"df":0,"a":{"docs":{},"df":0,"b":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":2.0}},"df":4}},"c":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":2.449489742783178}},"df":1}},"i":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.4142135623730951}},"df":1}},"t":{"docs":{},"df":0,"u":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.4142135623730951}},"df":2}}}},"r":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":2,"e":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":1,"f":{"docs":{},"df":0,"u":{"docs":{},"df":0,"l":{"docs":{},"df":0,"l":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":2}}}}}}},"t":{"docs":{},"df":0,"a":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{},"df":0,"r":{"docs":{},"df":0,"o":{"docs":{},"df":0,"p":{"docs":{},"df":0,"h":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":2.23606797749979},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":2.6457513110645907}},"df":2}}}}}}},"c":{"docs":{},"df":0,"h":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":2}}},"u":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":3,"a":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":3.1622776601683795}},"df":1}}}}},"b":{"docs":{},"df":0,"f":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":3.3166247903554}},"df":1,"'":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":1}}},"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.4142135623730951}},"df":2}}}},"r":{"docs":{},"df":0,"t":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.4142135623730951}},"df":4}}}}}},"f":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1}},"h":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.7320508075688772}},"df":1}},"l":{"docs":{},"df":0,"l":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"g":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":2.23606797749979},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.4142135623730951}},"df":9}}}}},"n":{"docs":{},"df":0,"g":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":2.23606797749979},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":5,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0}},"df":1}}}},"t":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0}},"df":1}},"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"k":{"docs":{"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":2}}},"o":{"docs":{},"df":0,"i":{"docs":{},"df":0,"c":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":3}},"o":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":2}},"s":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":1}}}}},"i":{"docs":{},"df":0,"t":{"docs":{},"df":0,"y":{"docs":{},"df":0,"s":{"docs":{},"df":0,"c":{"docs":{},"df":0,"a":{"docs":{},"df":0,"p":{"docs":{},"df":0,"e":{"docs":{},"df":0,"s":{"docs":{},"df":0,"+":{"docs":{},"df":0,"g":{"docs":{},"df":0,"t":{"docs":{},"df":0,"a":{"docs":{},"df":0,"5":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1}}}}}}}}}}}}}},"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":1}}}},"s":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":2,"i":{"docs":{},"df":0,"f":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":2.8284271247461903}},"df":1,"i":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.4142135623730951}},"df":2}}}}},"u":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":1}}},"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":3,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1}},"l":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0}},"df":1}}}},"v":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":5,"l":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":3}}}}}},"i":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.7320508075688772}},"df":1,"'":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.4142135623730951}},"df":1}}}},"p":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0}},"df":1}},"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":2.8284271247461903}},"df":1,"'":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":2.0}},"df":1}}},"s":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.4142135623730951}},"df":1}}},"u":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":3.605551275463989}},"df":1}}}}}},"o":{"docs":{},"df":0,"d":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":2.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":4.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":2.23606797749979}},"df":5,"b":{"docs":{},"df":0,"a":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":2}}}}},"e":{"docs":{},"df":0,"f":{"docs":{},"df":0,"f":{"docs":{},"df":0,"i":{"docs":{},"df":0,"c":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":1}}}}}},"l":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":3.4641016151377544}},"df":1,"'":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1},"l":{"docs":{},"df":0,"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":2}}}}},"m":{"docs":{},"df":0,"b":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":2.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.7320508075688772}},"df":5}},"o":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":1}},"e":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":2},"m":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{},"df":0,"s":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0}},"df":1}}}}}}},"p":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.4142135623730951}},"df":10},"t":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.4142135623730951}},"df":2}},"e":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":3}},"i":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":3.872983346207417}},"df":2}},"l":{"docs":{},"df":0,"e":{"docs":{},"df":0,"m":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":1}}}}}}},"t":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":3},"x":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":2.23606797749979},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":2.23606797749979},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":2.0}},"df":10}}},"o":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951}},"df":1}},"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"h":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951}},"df":3}}}}},"o":{"docs":{},"df":0,"m":{"docs":{},"df":0,"i":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":2}}}}},"u":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.7320508075688772}},"df":10,"a":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":6}}}}}}}}},"n":{"docs":{},"df":0,"c":{"docs":{},"df":0,"e":{"docs":{},"df":0,"p":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":3,"o":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":4.795831523312719}},"df":1}}}},"r":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":2}}},"i":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1}},"l":{"docs":{},"df":0,"u":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":14}}}},"d":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":2.0}},"df":1}},"u":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":3}}}},"f":{"docs":{},"df":0,"i":{"docs":{},"df":0,"r":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":3}}}},"n":{"docs":{},"df":0,"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1}}}},"s":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"v":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":1}}},"i":{"docs":{},"df":0,"d":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":1}}},"s":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":6}}},"t":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{},"df":0,"l":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1}}}}},"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.4142135623730951}},"df":1,"t":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.7320508075688772}},"df":2}}}},"u":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.4142135623730951}},"df":3}}}}},"u":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0}},"df":1}}},"t":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0}},"df":1}}},"e":{"docs":{},"df":0,"x":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":3}}},"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"u":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":2.449489742783178}},"df":2}}},"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":1}}},"i":{"docs":{},"df":0,"b":{"docs":{},"df":0,"u":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":2}}}},"o":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":2.23606797749979},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":2.8284271247461903},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":2.449489742783178},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":2.449489742783178}},"df":6}}}},"v":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":1}},"r":{"docs":{},"df":0,"g":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":2.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.4142135623730951}},"df":2}}}}},"o":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":3.4641016151377544}},"df":1,"'":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":3.0}},"df":1}}},"p":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0}},"df":1}},"r":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.4142135623730951}},"df":5},"p":{"docs":{},"df":0,"u":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":1}},"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.4142135623730951}},"df":4,"l":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":1}}}},"l":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":1},"s":{"docs":{},"df":0,"p":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":2}}}}}}}},"s":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":9}},"u":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":1}}}}},"v":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":2.8284271247461903}},"df":1}}},"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":1}}}},"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":5,"i":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":2}}}}}},"i":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"c":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":2.8284271247461903},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":3}}}},"o":{"docs":{},"df":0,"s":{"docs":{},"df":0,"s":{"docs":{},"df":0,"c":{"docs":{},"df":0,"o":{"docs":{},"df":0,"d":{"docs":{},"df":0,"e":{"docs":{},"df":0,"e":{"docs":{},"df":0,"v":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951}},"df":1}}}}}}}}},"u":{"docs":{},"df":0,"c":{"docs":{},"df":0,"i":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":7}}}}}},"u":{"docs":{},"df":0,"r":{"docs":{},"df":0,"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":3}}}},"v":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":1}}},"y":{"docs":{},"df":0,"b":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"t":{"docs":{},"df":0,"r":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":2.449489742783178}},"df":1,"'":{"docs":{"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.4142135623730951}},"df":1}}}}}}}}}},"d":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{},"df":0,"a":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":2.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":3.3166247903554},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":2.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.4142135623730951}},"df":10,"'":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0}},"df":1},"s":{"docs":{},"df":0,"e":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":2.23606797749979},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":2.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":2.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":11}}}}}},"d":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":2.23606797749979}},"df":1}},"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":3}},"c":{"docs":{},"df":0,"a":{"docs":{},"df":0,"y":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0}},"df":1}},"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1}}}},"i":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":1},"s":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":1}},"o":{"docs":{},"df":0,"m":{"docs":{},"df":0,"p":{"docs":{},"df":0,"o":{"docs":{},"df":0,"s":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":1}}}}}}},"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1}}}}},"e":{"docs":{},"df":0,"p":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":2,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":2}}}},"f":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.7320508075688772}},"df":1}}},"i":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":1,"i":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":2}}}}},"g":{"docs":{},"df":0,"r":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":1}}},"l":{"docs":{},"df":0,"v":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":2}},"m":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":2}}},"o":{"docs":{},"df":0,"n":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":8}}}}}},"p":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":2.449489742783178},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":6}}},"t":{"docs":{},"df":0,"h":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":2.0}},"df":2}}},"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"v":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":2.449489742783178}},"df":1}}},"s":{"docs":{},"df":0,"c":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"p":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0}},"df":1}}}}},"i":{"docs":{},"df":0,"g":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":11}},"r":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1}}},"t":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":4}}},"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":4.123105625617661},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":3,"o":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.4142135623730951}},"df":1}}}},"r":{"docs":{},"df":0,"m":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1}}}}}},"v":{"docs":{},"df":0,"e":{"docs":{},"df":0,"l":{"docs":{},"df":0,"o":{"docs":{},"df":0,"p":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":2.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":2.449489742783178},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":2.23606797749979}},"df":9}}}},"i":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":1}}}}},"i":{"docs":{},"df":0,"d":{"docs":{},"df":0,"n":{"docs":{},"df":0,"'":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":1}}}},"f":{"docs":{},"df":0,"f":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":2.23606797749979},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":2.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":10}},"i":{"docs":{},"df":0,"c":{"docs":{},"df":0,"u":{"docs":{},"df":0,"l":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":3,"i":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1}}}}}}}},"g":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0}},"df":1}}},"m":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0}},"df":1,"i":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":2}}}}}}},"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":13,"l":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":4}}}}},"i":{"docs":{},"df":0,"c":{"docs":{},"df":0,"h":{"docs":{},"df":0,"l":{"docs":{},"df":0,"e":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1}}}}}}},"s":{"docs":{},"df":0,"c":{"docs":{},"df":0,"o":{"docs":{},"df":0,"v":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0}},"df":1}},"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"m":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":1}}}}}}}}}},"u":{"docs":{},"df":0,"s":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0}},"df":1}}}},"r":{"docs":{},"df":0,"u":{"docs":{},"df":0,"p":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0}},"df":1}}}},"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":2.0}},"df":1},"n":{"docs":{},"df":0,"g":{"docs":{},"df":0,"u":{"docs":{},"df":0,"i":{"docs":{},"df":0,"s":{"docs":{},"df":0,"h":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":2}}}}}}},"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"b":{"docs":{},"df":0,"u":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":2.23606797749979},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.4142135623730951}},"df":2,"i":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{},"df":0,"s":{"docs":{},"df":0,"":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1}}}}}}}}}}}}}},"v":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":5,"r":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":4}}}}},"n":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.4142135623730951}},"df":1}},"o":{"docs":{},"df":0,"e":{"docs":{},"df":0,"s":{"docs":{},"df":0,"n":{"docs":{},"df":0,"'":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0}},"df":1}}}}},"m":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":2.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":2.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":3.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.7320508075688772}},"df":5}}}},"n":{"docs":{},"df":0,"'":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":2}}},"r":{"docs":{},"df":0,"a":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0}},"df":1}},"w":{"docs":{},"df":0,"n":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{},"df":0,"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":1}}}}}}}}},"p":{"docs":{},"df":0,"o":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0}},"df":1}},"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"f":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.4142135623730951}},"df":1}},"w":{"docs":{},"df":0,"b":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"k":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0}},"df":1}}}}}},"i":{"docs":{},"df":0,"v":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":1,"n":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0}},"df":1}}}},"o":{"docs":{},"df":0,"p":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":2}}},"s":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.4142135623730951}},"df":2}},"u":{"docs":{},"df":0,"b":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":1}}},"r":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.4142135623730951}},"df":3}}},"y":{"docs":{},"df":0,"n":{"docs":{},"df":0,"a":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":2.8284271247461903},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":2.0}},"df":3}}}}},"e":{"docs":{},"df":0,".":{"docs":{},"df":0,"g":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.4142135623730951}},"df":1}},"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"h":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":4}},"r":{"docs":{},"df":0,"l":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":1}}},"s":{"docs":{},"df":0,"i":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":1}}}}},"c":{"docs":{},"df":0,"o":{"docs":{},"df":0,"s":{"docs":{},"df":0,"y":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1}}}}}}}},"d":{"docs":{},"df":0,"g":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951}},"df":1}},"f":{"docs":{},"df":0,"f":{"docs":{},"df":0,"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":2.23606797749979},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":2.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":2.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":2.6457513110645907},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":12}}},"i":{"docs":{},"df":0,"c":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":2.449489742783178},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":2.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":2.449489742783178},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":2.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":2.6457513110645907},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":2.0}},"df":13}}}}},"l":{"docs":{},"df":0,"e":{"docs":{},"df":0,"g":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{},"df":0,"l":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0}},"df":1}}}}}},"m":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1}}}}},"i":{"docs":{},"df":0,"m":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1}}}},"l":{"docs":{},"df":0,"i":{"docs":{},"df":0,"p":{"docs":{},"df":0,"s":{"docs":{},"df":0,"o":{"docs":{},"df":0,"i":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":1}}}}}}}},"m":{"docs":{},"df":0,"b":{"docs":{},"df":0,"e":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":1}}},"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"g":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":2}}},"p":{"docs":{},"df":0,"h":{"docs":{},"df":0,"a":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":1}}},"i":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.7320508075688772}},"df":1}},"o":{"docs":{},"df":0,"w":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1}}}},"n":{"docs":{},"df":0,"a":{"docs":{},"df":0,"b":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":3}}},"c":{"docs":{},"df":0,"a":{"docs":{},"df":0,"p":{"docs":{},"df":0,"s":{"docs":{},"df":0,"u":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0}},"df":1}}}}},"o":{"docs":{},"df":0,"m":{"docs":{},"df":0,"p":{"docs":{},"df":0,"a":{"docs":{},"df":0,"s":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1}}}}},"u":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"g":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":2}}}}}},"g":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":3.1622776601683795},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":2.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":3}}},"h":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"c":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":2.23606797749979},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":7}}}},"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0}},"df":1}}},"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"c":{"docs":{},"df":0,"h":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.7320508075688772}},"df":1}}}},"s":{"docs":{},"df":0,"e":{"docs":{},"df":0,"m":{"docs":{},"df":0,"b":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":1}}}},"u":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":6}}},"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951}},"df":2}}},"u":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":1}},"v":{"docs":{},"df":0,"i":{"docs":{},"df":0,"r":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":1}}}}}},"r":{"docs":{},"df":0,"a":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":1},"r":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":3}}}},"s":{"docs":{},"df":0,"p":{"docs":{},"df":0,"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":7}}}},"s":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0}},"df":1}}}}},"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":1}}}},"t":{"docs":{},"df":0,"c":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":2}},"v":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{},"df":0,"u":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":2.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":2.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":9}}},"e":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.7320508075688772}},"df":7}},"i":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":2}},"o":{"docs":{},"df":0,"l":{"docs":{},"df":0,"v":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":2}}}},"x":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{},"df":0,"l":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0}},"df":1}}}},"m":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":1}},"p":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.4142135623730951}},"df":4}}}},"c":{"docs":{},"df":0,"e":{"docs":{},"df":0,"e":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0}},"df":1}},"l":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.4142135623730951}},"df":1},"p":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":1}}}}}},"i":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":7}}},"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"u":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":1}}}},"h":{"docs":{},"df":0,"a":{"docs":{},"df":0,"u":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":1}}}},"i":{"docs":{},"df":0,"b":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":3}}}}},"i":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.7320508075688772}},"df":10}}},"p":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":4},"s":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":2}}},"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":6}},"r":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":3.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.7320508075688772}},"df":7,"m":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":4}}}}},"t":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0}},"df":1}}},"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":2.0}},"df":1}},"n":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":4.242640687119285}},"df":2}},"i":{"docs":{},"df":0,"c":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":1}}}},"o":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":2.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.4142135623730951}},"df":12}}},"o":{"docs":{},"df":0,"n":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":1}}}}}}},"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":6},"s":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.4142135623730951}},"df":3}},"r":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.4142135623730951}},"df":1}}},"r":{"docs":{},"df":0,"a":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":1,"c":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1}}},"e":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":4}}}}},"y":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1}},"f":{"docs":{},"df":0,"a":{"docs":{},"df":0,"b":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"c":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":1}}}},"c":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1},"i":{"docs":{},"df":0,"l":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":1}}}},"t":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":1,"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":2.449489742783178}},"df":1}}}}},"i":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":1}},"l":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":3},"s":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.4142135623730951}},"df":1}},"r":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":2},"s":{"docs":{},"df":0,"c":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.4142135623730951}},"df":1}}},"t":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.4142135623730951}},"df":1,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.7320508075688772}},"df":1}}}}},"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{},"df":0,"u":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":3.1622776601683795},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":4}}}},"d":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1,"a":{"docs":{},"df":0,"v":{"docs":{},"df":0,"g":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":2.23606797749979}},"df":1}}},"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":3.4641016151377544}},"df":1}}},"e":{"docs":{},"df":0,"d":{"docs":{},"df":0,"b":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"k":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.7320508075688772}},"df":3}}}}}}},"i":{"docs":{},"df":0,"e":{"docs":{},"df":0,"l":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":4}}},"g":{"docs":{},"df":0,"u":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0}},"df":1}}},"l":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951}},"df":2},"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951}},"df":1}}}},"n":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":3}},"d":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":2.0}},"df":7},"e":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":2.23606797749979},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":3.1622776601683795},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.4142135623730951}},"df":6}},"r":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":1}}},"v":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":1}},"x":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1}},"l":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.7320508075688772}},"df":1,"e":{"docs":{},"df":0,"x":{"docs":{},"df":0,"i":{"docs":{},"df":0,"b":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1}}}}}},"o":{"docs":{},"df":0,"c":{"docs":{},"df":0,"u":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":5,"s":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.4142135623730951}},"df":6}}},"l":{"docs":{},"df":0,"l":{"docs":{},"df":0,"o":{"docs":{},"df":0,"w":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0}},"df":1}}}},"o":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":1},"t":{"docs":{},"df":0,"p":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":1}}}}}}},"r":{"docs":{},"df":0,"g":{"docs":{},"df":0,"e":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":2.449489742783178},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":3.605551275463989}},"df":2}}},"m":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0}},"df":1,"a":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":2.23606797749979}},"df":1},"t":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951}},"df":1}},"u":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":1}}},"w":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":5}}}}},"u":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":2}}}},"r":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":2}}},"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"m":{"docs":{},"df":0,"e":{"docs":{},"df":0,"w":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"k":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":2.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":2.449489742783178},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":6,"'":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0}},"df":1}}}}}}}},"e":{"docs":{},"df":0,"q":{"docs":{},"df":0,"u":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":3}}}}}},"u":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0}},"df":1}}}}}}},"u":{"docs":{},"df":0,"l":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.4142135623730951}},"df":3,"i":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":4}}},"n":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":2.449489742783178}},"df":8}}}}}},"r":{"docs":{},"df":0,"t":{"docs":{},"df":0,"h":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":2.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":2.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.4142135623730951}},"df":13,"m":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0}},"df":1}}}}}}}},"t":{"docs":{},"df":0,"u":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":2.0},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":2.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":2.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":2.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":2.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.4142135623730951}},"df":15}}},"z":{"docs":{},"df":0,"z":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":1}}}},"g":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.4142135623730951}},"df":2}},"m":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":2}},"u":{"docs":{},"df":0,"s":{"docs":{},"df":0,"s":{"docs":{},"df":0,"i":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1}}}}}}},"e":{"docs":{},"df":0,"m":{"docs":{},"df":0,"m":{"docs":{},"df":0,"a":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":1}}},"n":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":3.605551275463989},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":4.242640687119285},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":2.8284271247461903},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":2.23606797749979},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":10,"a":{"docs":{},"df":0,"l":{"docs":{},"df":0,"i":{"docs":{},"df":0,"z":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":8}}}}}}},"t":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0}},"df":1}},"i":{"docs":{},"df":0,"v":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":1}}}},"l":{"docs":{},"df":0,"i":{"docs":{},"df":0,"m":{"docs":{},"df":0,"p":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0}},"df":1}}}},"o":{"docs":{},"df":0,"b":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1}}}}},"m":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1}},"o":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.4142135623730951}},"df":4}}},"p":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.4142135623730951}},"df":2}},"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"d":{"docs":{},"df":0,"i":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":3}}}}},"i":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":2}},"p":{"docs":{},"df":0,"h":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":3.0}},"df":1}}},"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":3,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":2}},"l":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1}}}}},"i":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":1}},"o":{"docs":{},"df":0,"u":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{},"df":0,"b":{"docs":{},"df":0,"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"k":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":4}}}}}}},"p":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1}}}},"s":{"docs":{},"df":0,"m":{"docs":{},"df":0,"8":{"docs":{},"df":0,"k":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.4142135623730951}},"df":1}}}},"u":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":1}}}}}},"i":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":2}}}},"h":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":1,"l":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":5}}},"p":{"docs":{},"df":0,"p":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":1}}}},"r":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":1,"m":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":1}},"v":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":2}}},"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":3},"t":{"docs":{},"df":0,"m":{"docs":{},"df":0,"a":{"docs":{},"df":0,"p":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":1}}}},"v":{"docs":{},"df":0,"i":{"docs":{},"df":0,"l":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0}},"df":1}}},"y":{"docs":{},"df":0,"w":{"docs":{},"df":0,"e":{"docs":{},"df":0,"i":{"docs":{},"df":0,"g":{"docs":{},"df":0,"h":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":1}}}}}}}}},"l":{"docs":{},"df":0,"p":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":3}},"r":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1,"'":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0}},"df":1}}},"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"o":{"docs":{},"df":0,"g":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1}}}}}}},"u":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1}}}}}},"i":{"docs":{},"df":0,"d":{"docs":{},"df":0,"d":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":2}}}},"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"c":{"docs":{},"df":0,"h":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":1}}}}}},"g":{"docs":{},"df":0,"h":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.7320508075688772}},"df":3,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":2.0}},"df":3}},"l":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.4142135623730951}},"df":4,"g":{"docs":{},"df":0,"h":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":5}}}}}}},"n":{"docs":{},"df":0,"d":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1}}}}},"o":{"docs":{},"df":0,"l":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":1},"i":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1}}}},"m":{"docs":{},"df":0,"o":{"docs":{},"df":0,"g":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1}}}}}},"u":{"docs":{},"df":0,"g":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0}},"df":1},"g":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"g":{"docs":{},"df":0,"g":{"docs":{},"df":0,"p":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":1}}}}}}}},"m":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":2.0}},"df":3}}},"n":{"docs":{},"df":0,"g":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":1}}}}}}},"r":{"docs":{},"df":0,"d":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":2}},"t":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":1}},"s":{"docs":{},"df":0,"k":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":2.23606797749979}},"df":1}}}},"y":{"docs":{},"df":0,"p":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"p":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"m":{"docs":{},"df":0,"e":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.4142135623730951}},"df":1}}}}}}}}},"o":{"docs":{},"df":0,"t":{"docs":{},"df":0,"h":{"docs":{},"df":0,"e":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":3,"i":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.4142135623730951}},"df":1}}}}}}}}},"i":{"docs":{},"df":0,"b":{"docs":{},"df":0,"p":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":1}},"d":{"docs":{},"df":0,"e":{"docs":{},"df":0,"a":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":3},"n":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1,"i":{"docs":{},"df":0,"f":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":5}}}}}}},"i":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":2.6457513110645907}},"df":1}},"l":{"docs":{},"df":0,"l":{"docs":{},"df":0,"u":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":2}}}}}},"m":{"docs":{},"df":0,"a":{"docs":{},"df":0,"g":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":3.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":2.6457513110645907}},"df":4,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"e":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.4142135623730951}},"df":1}}}},"i":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951}},"df":1}}}},"m":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1}}}},"p":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":2.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.4142135623730951}},"df":6}}},"l":{"docs":{},"df":0,"e":{"docs":{},"df":0,"m":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0}},"df":1}}}}},"i":{"docs":{"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":1}},"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":3,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{},"df":0,"l":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":3}}}}}}},"s":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":1}}},"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"s":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.4142135623730951}},"df":10}}},"o":{"docs":{},"df":0,"v":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":2.23606797749979},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":2.8284271247461903},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":3.7416573867739413},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":2.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":15,"e":{"docs":{},"df":0,"m":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{},"df":0,"":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{},"df":0,"'":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1}}}}}}}}}}}}}},"n":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"c":{"docs":{},"df":0,"u":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":1}}}}}}}},"c":{"docs":{},"df":0,"l":{"docs":{},"df":0,"u":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":2.0}},"df":4}}},"o":{"docs":{},"df":0,"n":{"docs":{},"df":0,"s":{"docs":{},"df":0,"i":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":1}}}}},"r":{"docs":{},"df":0,"p":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":4}}}}},"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":6}},"d":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0}},"df":1},"m":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1}}}}}}},"d":{"docs":{},"df":0,"e":{"docs":{},"df":0,"p":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":2}}}}},"i":{"docs":{},"df":0,"c":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":2},"v":{"docs":{},"df":0,"i":{"docs":{},"df":0,"d":{"docs":{},"df":0,"u":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":3}}}}}},"e":{"docs":{},"df":0,"f":{"docs":{},"df":0,"f":{"docs":{},"df":0,"i":{"docs":{},"df":0,"c":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1}}}}},"x":{"docs":{},"df":0,"p":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":1}}}}}},"f":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.7320508075688772}},"df":2}},"l":{"docs":{},"df":0,"u":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"c":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.4142135623730951}},"df":2}}}}},"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":3}}}},"h":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0}},"df":2}}}}},"i":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":3.1622776601683795},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":5}}},"n":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":1}},"o":{"docs":{},"df":0,"v":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":3}}},"p":{"docs":{},"df":0,"u":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":2.8284271247461903},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":2.23606797749979},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.7320508075688772}},"df":6}}},"s":{"docs":{},"df":0,"i":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":2},"g":{"docs":{},"df":0,"h":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":2.0}},"df":4}}}},"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":6}}},"r":{"docs":{},"df":0,"u":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.7320508075688772}},"df":1}}}}}},"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"g":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":2.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":6}},"l":{"docs":{},"df":0,"l":{"docs":{},"df":0,"i":{"docs":{},"df":0,"g":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.4142135623730951}},"df":3}}}},"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":2}}},"c":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{},"df":0,"n":{"docs":{},"df":0,"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1,"e":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1}}}}}}}}},"n":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":2.0}},"df":1},"p":{"docs":{},"df":0,"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":3.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":5}}}},"s":{"docs":{},"df":0,"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1}}}},"v":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.4142135623730951}},"df":1,"e":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.4142135623730951}},"df":1,"t":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0}},"df":2}}}}}},"r":{"docs":{},"df":0,"o":{"docs":{},"df":0,"d":{"docs":{},"df":0,"u":{"docs":{},"df":0,"c":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":13,"t":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":12}}}}}}},"v":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.7320508075688772}},"df":1}},"s":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"g":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":9}}}}},"o":{"docs":{},"df":0,"c":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1},"l":{"docs":{},"df":0,"v":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":4}}}}},"r":{"docs":{},"df":0,"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"l":{"docs":{},"df":0,"e":{"docs":{},"df":0,"v":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1}}}}}},"s":{"docs":{},"df":0,"n":{"docs":{},"df":0,"'":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":2}}},"o":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":2.449489742783178},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":3}},"s":{"docs":{},"df":0,"u":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.7320508075688772}},"df":1}}},"t":{"docs":{},"df":0,"'":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":5},"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":3}}}},"j":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":1,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"o":{"docs":{},"df":0,"b":{"docs":{},"df":0,"i":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":1}}}}}},"i":{"docs":{},"df":0,"l":{"docs":{},"df":0,"b":{"docs":{},"df":0,"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"k":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.7320508075688772}},"df":1}}}}}}}},"o":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.7320508075688772}},"df":1}}}}},"k":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":2,"e":{"docs":{},"df":0,"e":{"docs":{},"df":0,"p":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":1}},"y":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.4142135623730951}},"df":8}},"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"e":{"docs":{},"df":0,"m":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0}},"df":1}}}}}},"n":{"docs":{},"df":0,"o":{"docs":{},"df":0,"w":{"docs":{},"df":0,"l":{"docs":{},"df":0,"e":{"docs":{},"df":0,"d":{"docs":{},"df":0,"g":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":2.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.4142135623730951}},"df":3}}}}}}}},"l":{"docs":{},"df":0,"2":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0}},"df":1},"a":{"docs":{},"df":0,"b":{"docs":{},"df":0,"e":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.7320508075688772}},"df":1}}},"c":{"docs":{},"df":0,"k":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":2}},"n":{"docs":{},"df":0,"g":{"docs":{},"df":0,"u":{"docs":{},"df":0,"a":{"docs":{},"df":0,"g":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":2.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":3.1622776601683795}},"df":12}}}}},"r":{"docs":{},"df":0,"g":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.4142135623730951}},"df":11,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":3}}}},"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":3.1622776601683795}},"df":1}}}},"y":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":3}}}},"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.7320508075688772}},"df":7},"p":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":5},"r":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":2.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":3.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":8}}},"d":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":2},"n":{"docs":{},"df":0,"g":{"docs":{},"df":0,"t":{"docs":{},"df":0,"h":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":2}}}},"s":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":3}},"t":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0}},"df":1,"'":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":1}},"v":{"docs":{},"df":0,"e":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":2.23606797749979},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":2.449489742783178}},"df":3},"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"g":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":4}}}}}},"i":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":3,"b":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0}},"df":1}}}}},"e":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.7320508075688772}},"df":1},"g":{"docs":{},"df":0,"h":{"docs":{},"df":0,"t":{"docs":{},"df":0,"w":{"docs":{},"df":0,"e":{"docs":{},"df":0,"i":{"docs":{},"df":0,"g":{"docs":{},"df":0,"h":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":2}}}}}}}}},"m":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":2.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.4142135623730951}},"df":12}}},"n":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":2.0}},"df":1,"a":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":2.0}},"df":3}}},"k":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":2}}},"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"m":{"docs":{},"df":0,"a":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":3}}},"m":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":4.58257569495584},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":3.4641016151377544},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":3.605551275463989},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":3.4641016151377544},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":4.358898943540674},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":2.8284271247461903},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":4.123105625617661},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":3.4641016151377544},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":12,"'":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":2.449489742783178}},"df":3},"s":{"docs":{},"df":0,"c":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":3.605551275463989}},"df":1}}}}}},"o":{"docs":{},"df":0,"c":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.4142135623730951}},"df":1}}},"g":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{},"df":0,"h":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":2.0}},"df":1}}}}}},"i":{"docs":{},"df":0,"c":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.7320508075688772}},"df":1}}},"n":{"docs":{},"df":0,"g":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":2}},"o":{"docs":{},"df":0,"k":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":3}},"r":{"docs":{},"df":0,"a":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":2.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":2.0}},"df":2}},"s":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0}},"df":2},"s":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":2.23606797749979},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":3}},"w":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.7320508075688772}},"df":2,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.4142135623730951}},"df":2}}}}},"m":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"h":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":2}}}},"d":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":1}},"g":{"docs":{},"df":0,"i":{"docs":{},"df":0,"c":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":3}}},"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":4}}},"e":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0}},"df":1}}}}},"j":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":2}}},"k":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":9}},"n":{"docs":{},"df":0,"a":{"docs":{},"df":0,"g":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":1}},"i":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":5,"p":{"docs":{},"df":0,"u":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":2}}}},"n":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0}},"df":1}}},"u":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":1}}}},"p":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":2.0}},"df":1},"t":{"docs":{},"df":0,"c":{"docs":{},"df":0,"h":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":3}},"h":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0}},"df":1,"e":{"docs":{},"df":0,"m":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":3}}}}},"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"c":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.4142135623730951}},"df":3},"x":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.7320508075688772}},"df":1}}},"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":2}}}},"x":{"docs":{},"df":0,"i":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":1}}}},"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.4142135623730951}},"df":4},"s":{"docs":{},"df":0,"u":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":4}}}},"c":{"docs":{},"df":0,"h":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.7320508075688772}},"df":5}}}},"d":{"docs":{},"df":0,"i":{"docs":{},"df":0,"c":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":1}}},"m":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":2.0}},"df":1}}}},"n":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0}},"df":1}}}}},"r":{"docs":{},"df":0,"g":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0}},"df":1}},"s":{"docs":{},"df":0,"h":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":2.449489742783178}},"df":1}},"t":{"docs":{},"df":0,"a":{"docs":{},"df":0,"d":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{},"df":0,"a":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":1}}}}},"h":{"docs":{},"df":0,"o":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":2.8284271247461903},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":3.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":2.23606797749979},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":2.449489742783178},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":2.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":2.8284271247461903}},"df":14,"o":{"docs":{},"df":0,"l":{"docs":{},"df":0,"o":{"docs":{},"df":0,"g":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":3}}}}}}},"i":{"docs":{},"df":0,"c":{"docs":{},"df":0,"u":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.4142135623730951}},"df":1}}}},"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"c":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":3}}}}},"i":{"docs":{},"df":0,"l":{"docs":{},"df":0,"l":{"docs":{},"df":0,"i":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":2.449489742783178}},"df":1}}}},"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0}},"df":1}}}},"n":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":2.23606797749979}},"df":1,"m":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":3}}},"o":{"docs":{},"df":0,"u":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1}},"s":{"docs":{},"df":0,"b":{"docs":{},"df":0,"e":{"docs":{},"df":0,"h":{"docs":{},"df":0,"a":{"docs":{},"df":0,"v":{"docs":{},"df":0,"i":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":3.1622776601683795}},"df":1}}}}}}}},"s":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951}},"df":1},"t":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0}},"df":1}}}}},"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"g":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":5}}},"x":{"docs":{},"df":0,"t":{"docs":{},"df":0,"u":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1}}}}},"l":{"docs":{},"df":0,"p":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":1}},"o":{"docs":{},"df":0,"b":{"docs":{},"df":0,"i":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951}},"df":1}}},"d":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":2}},"e":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":3.4641016151377544},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":2.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":2.6457513110645907},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":2.23606797749979},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":3.1622776601683795},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":2.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":3.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":2.449489742783178},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":2.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":2.23606797749979},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.7320508075688772}},"df":13,"'":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":4}},"r":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":1}}},"i":{"docs":{},"df":0,"f":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":1}}},"u":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1,"a":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1}}}}},"n":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.4142135623730951}},"df":2}}}}},"r":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":2.23606797749979},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":2.23606797749979},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":2.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":2.6457513110645907},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":2.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":2.23606797749979},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":2.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":2.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":2.8284271247461903},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.7320508075688772}},"df":14}},"v":{"docs":{},"df":0,"e":{"docs":{},"df":0,"m":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0}},"df":1}}}}}}},"t":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.7320508075688772}},"df":1},"u":{"docs":{},"df":0,"c":{"docs":{},"df":0,"h":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":1}},"l":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":1,"f":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"e":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":1}}}}},"m":{"docs":{},"df":0,"o":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":1}}},"p":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.4142135623730951}},"df":5}}}}}}},"n":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{},"df":0,"u":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":4}}}},"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"e":{"docs":{},"df":0,"s":{"docs":{},"df":0,"s":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1}}},"i":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1}}}}}},"e":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":9}},"g":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.4142135623730951}},"df":2},"o":{"docs":{},"df":0,"x":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":1}},"t":{"docs":{},"df":0,"w":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"k":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":3.3166247903554},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":2.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":2.0}},"df":4}}}}},"u":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":3.605551275463989},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":2.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":3.605551275463989}},"df":4}},"o":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":2.0}},"df":1}}}},"w":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":2.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":10},"x":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":1}}},"l":{"docs":{},"df":0,"p":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0}},"df":1}},"n":{"docs":{},"df":0,"c":{"docs":{},"df":0,"b":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":1}},"f":{"docs":{},"df":0,"c":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":3.4641016151377544}},"df":1}}},"o":{"docs":{},"df":0,"d":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951}},"df":1}},"n":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":2.6457513110645907},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":3},"r":{"docs":{},"df":0,"m":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":1}}}},"v":{"docs":{},"df":0,"e":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.7320508075688772}},"df":10}}}},"u":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"c":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":1}}},"l":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.4142135623730951}},"df":1}},"m":{"docs":{},"df":0,"b":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":2}}},"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":1}}}}},"o":{"docs":{},"df":0,"b":{"docs":{},"df":0,"j":{"docs":{},"df":0,"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":2.6457513110645907},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":4}}}},"s":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"v":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0}},"df":1}}}}},"f":{"docs":{},"df":0,"f":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":1}},"r":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.4142135623730951}},"df":11}}}},"n":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":1},"p":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":5},"r":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.7320508075688772}},"df":3}},"p":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"t":{"docs":{},"df":0,"u":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":2}}}}}},"t":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":1,"i":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":8}}}},"r":{"docs":{},"df":0,"g":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951}},"df":2}}},"i":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.4142135623730951}},"df":1}}},"g":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0}},"df":1}}}},"t":{"docs":{},"df":0,"h":{"docs":{},"df":0,"o":{"docs":{},"df":0,"g":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0}},"df":1}}}}}}},"u":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":2,"c":{"docs":{},"df":0,"o":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1}}},"l":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1}}},"p":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"f":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.4142135623730951}},"df":9}}}}}},"u":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":2.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":2.449489742783178},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.7320508075688772}},"df":3}}},"s":{"docs":{},"df":0,"h":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":1}}}}}},"v":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.4142135623730951}},"df":5,"a":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0}},"df":1}},"c":{"docs":{},"df":0,"o":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":3}}},"f":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":1}}},"l":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":1},"o":{"docs":{},"df":0,"o":{"docs":{},"df":0,"k":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":1}}}},"w":{"docs":{},"df":0,"h":{"docs":{},"df":0,"e":{"docs":{},"df":0,"l":{"docs":{},"df":0,"m":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"g":{"docs":{},"df":0,"l":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1}}}}}}}}}}}}}},"p":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1}},"p":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.4142135623730951}},"df":12,"'":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0}},"df":1}}}},"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"d":{"docs":{},"df":0,"i":{"docs":{},"df":0,"g":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1}}}},"m":{"docs":{},"df":0,"e":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":2.0}},"df":4,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":1}}}},"o":{"docs":{},"df":0,"u":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":1}}}}}},"s":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.7320508075688772}},"df":1},"t":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":3,"i":{"docs":{},"df":0,"c":{"docs":{},"df":0,"i":{"docs":{},"df":0,"p":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1}},"u":{"docs":{},"df":0,"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":1,"l":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.4142135623730951}},"df":4}}}}}}}},"n":{"docs":{},"df":0,"e":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951}},"df":1}}}}},"t":{"docs":{},"df":0,"h":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0}},"df":1,"w":{"docs":{},"df":0,"a":{"docs":{},"df":0,"y":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":1}}}},"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.4142135623730951}},"df":2}}}}},"v":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":7}}},"e":{"docs":{},"df":0,"e":{"docs":{},"df":0,"k":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":1}},"f":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":1}},"r":{"docs":{},"df":0,"c":{"docs":{},"df":0,"e":{"docs":{},"df":0,"p":{"docs":{},"df":0,"t":{"docs":{},"df":0,"r":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":1}}}}}}},"f":{"docs":{},"df":0,"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{},"df":0,"l":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":1}}}}},"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":2.23606797749979},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":2.449489742783178},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":2.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":3.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":2.23606797749979},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.4142135623730951}},"df":13}}}},"p":{"docs":{},"df":0,"l":{"docs":{},"df":0,"e":{"docs":{},"df":0,"x":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":1}}}},"s":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0}},"df":2}}},"t":{"docs":{},"df":0,"u":{"docs":{},"df":0,"r":{"docs":{},"df":0,"b":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":1}}}}},"s":{"docs":{},"df":0,"k":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":1}}}},"h":{"docs":{},"df":0,"a":{"docs":{},"df":0,"s":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1}}}},"i":{"docs":{},"df":0,"c":{"docs":{},"df":0,"k":{"docs":{"http://127.0.0.1:1111/blog/":{"tf":1.0}},"df":1}},"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"e":{"docs":{},"df":0,"w":{"docs":{},"df":0,"i":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":1}}}}}},"l":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":1}},"p":{"docs":{},"df":0,"e":{"docs":{},"df":0,"l":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.4142135623730951}},"df":1}}}}},"s":{"docs":{},"df":0,"s":{"docs":{},"df":0,"a":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0}},"df":1}}},"t":{"docs":{},"df":0,"f":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0}},"df":1}}}}},"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951}},"df":1,"m":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.7320508075688772}},"df":1}}}}}},"n":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1,"a":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":1}}}},"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":1}}}},"u":{"docs":{},"df":0,"g":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1}}},"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":1}}}}},"o":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":1}}},"l":{"docs":{},"df":0,"i":{"docs":{},"df":0,"c":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0}},"df":1}}}},"p":{"docs":{},"df":0,"u":{"docs":{},"df":0,"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":3}}}}},"s":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":1},"s":{"docs":{},"df":0,"i":{"docs":{},"df":0,"b":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.4142135623730951}},"df":5}}}},"t":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":6}},"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":2.449489742783178},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":11}}}}},"w":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":7}}}},"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"c":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":2},"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":1}}}}}}}},"e":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1,"c":{"docs":{},"df":0,"i":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.4142135623730951}},"df":3}}},"d":{"docs":{},"df":0,"i":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.4142135623730951}},"df":2}}}},"f":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0}},"df":1}}},"s":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.4142135623730951}},"df":5}},"r":{"docs":{},"df":0,"v":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.4142135623730951}},"df":1}}}},"t":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1}}}}},"v":{"docs":{},"df":0,"i":{"docs":{},"df":0,"o":{"docs":{},"df":0,"u":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.4142135623730951}},"df":3,"s":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":2}}}}}},"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"c":{"docs":{},"df":0,"i":{"docs":{},"df":0,"p":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0}},"df":1}}}}},"o":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":1}},"v":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1}}}}},"o":{"docs":{},"df":0,"b":{"docs":{},"df":0,"l":{"docs":{},"df":0,"e":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.7320508075688772}},"df":7}}}},"c":{"docs":{},"df":0,"e":{"docs":{},"df":0,"d":{"docs":{},"df":0,"u":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1}}},"s":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":2.449489742783178},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":2.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":7}}}},"d":{"docs":{},"df":0,"u":{"docs":{},"df":0,"c":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":2}}},"g":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":3.872983346207417},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.7320508075688772}},"df":2}}}},"j":{"docs":{},"df":0,"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":2.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":3}}}},"m":{"docs":{},"df":0,"i":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.4142135623730951}},"df":11}},"p":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.4142135623730951}},"df":4}}},"n":{"docs":{},"df":0,"o":{"docs":{},"df":0,"u":{"docs":{},"df":0,"n":{"docs":{},"df":0,"c":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":2}}}}},"o":{"docs":{},"df":0,"f":{"docs":{"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":1}},"p":{"docs":{},"df":0,"a":{"docs":{},"df":0,"g":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":2.8284271247461903}},"df":1}},"o":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":3}},"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"e":{"docs":{},"df":0,"t":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0}},"df":1}}}}}}}},"v":{"docs":{},"df":0,"a":{"docs":{},"df":0,"b":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.4142135623730951}},"df":1}}},"e":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":4,"n":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0}},"df":1}},"i":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":2.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":8}}}},"u":{"docs":{},"df":0,"n":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.7320508075688772}},"df":1}}}},"u":{"docs":{},"df":0,"b":{"docs":{},"df":0,"l":{"docs":{},"df":0,"i":{"docs":{},"df":0,"c":{"docs":{},"df":0,"l":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":1}}}}}},"t":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":2}},"y":{"docs":{},"df":0,"t":{"docs":{},"df":0,"h":{"docs":{},"df":0,"i":{"docs":{},"df":0,"a":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":1}},"o":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":2.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":3}}}}}},"q":{"docs":{},"df":0,"l":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":3.0}},"df":1,"'":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.4142135623730951}},"df":1}}}}},"u":{"docs":{},"df":0,"a":{"docs":{},"df":0,"d":{"docs":{},"df":0,"r":{"docs":{},"df":0,"o":{"docs":{},"df":0,"t":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":1}}}}}},"i":{"docs":{},"df":0,"l":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":3.872983346207417}},"df":1,"'":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.4142135623730951}},"df":1}}}}}},"l":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":2.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":2.6457513110645907},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.4142135623730951}},"df":5}}}},"n":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"z":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":4.0}},"df":1}}}}}}},"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"d":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":3.7416573867739413}},"df":1,"s":{"docs":{},"df":0,"'":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0}},"df":1}}}},"n":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":1,"d":{"docs":{},"df":0,"o":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0}},"df":1}}},"g":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":3},"k":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":2.23606797749979},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.7320508075688772}},"df":2}},"p":{"docs":{},"df":0,"i":{"docs":{},"df":0,"d":{"docs":{},"df":0,"l":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":2}}}}},"t":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.7320508075688772}},"df":4},"i":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":4.58257569495584}},"df":1}}}}}}},"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"d":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":2}},"l":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":2.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.7320508075688772}},"df":7,"i":{"docs":{},"df":0,"s":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0}},"df":1},"t":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":3}}}},"s":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":5}}}},"c":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":2}}},"o":{"docs":{},"df":0,"g":{"docs":{},"df":0,"n":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":1}}}}},"u":{"docs":{},"df":0,"r":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":1}}}},"d":{"docs":{},"df":0,"p":{"docs":{},"df":0,"a":{"docs":{},"df":0,"j":{"docs":{},"df":0,"a":{"docs":{},"df":0,"m":{"docs":{},"df":0,"a":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":1}}}}}},"u":{"docs":{},"df":0,"c":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":5,"t":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.4142135623730951}},"df":1}}}},"f":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1}}}}},"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":2}},"i":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":7}},"l":{"docs":{},"df":0,"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1}}}}},"g":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"d":{"docs":{},"df":0,"l":{"docs":{},"df":0,"e":{"docs":{},"df":0,"s":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":1}}}}}}},"i":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":1}}},"u":{"docs":{},"df":0,"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":2.8284271247461903}},"df":1}}}}},"i":{"docs":{},"df":0,"g":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":1}}},"l":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":2,"a":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":2,"i":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{},"df":0,"s":{"docs":{},"df":0,"h":{"docs":{},"df":0,"i":{"docs":{},"df":0,"p":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.7320508075688772}},"df":1}}}}}}}},"x":{"docs":{"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":1}},"e":{"docs":{},"df":0,"v":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":2}},"i":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":5,"a":{"docs":{},"df":0,"b":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":2.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.4142135623730951}},"df":5}},"n":{"docs":{},"df":0,"c":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":2}}}},"u":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.4142135623730951}},"df":1}},"m":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":4}},"r":{"docs":{},"df":0,"k":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":2}}}},"p":{"docs":{},"df":0,"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0}},"df":1}}},"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"y":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0}},"df":1}}},"o":{"docs":{},"df":0,"g":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"p":{"docs":{},"df":0,"h":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":4.58257569495584}},"df":1,"'":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.7320508075688772}},"df":1}}}}}},"s":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":2.8284271247461903}},"df":1}}}}}}},"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":2.23606797749979},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":7,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":3}}}}}}},"q":{"docs":{},"df":0,"u":{"docs":{},"df":0,"i":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.4142135623730951}},"df":9}}}},"s":{"docs":{},"df":0,"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"c":{"docs":{},"df":0,"h":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":2.6457513110645907},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":2.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":2.449489742783178},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":2.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":2.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":2.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":2.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":2.6457513110645907},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":2.449489742783178},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":2.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":2.6457513110645907},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":2.23606797749979}},"df":15}}}}},"i":{"docs":{},"df":0,"d":{"docs":{},"df":0,"u":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":1}}},"o":{"docs":{},"df":0,"l":{"docs":{},"df":0,"u":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.4142135623730951}},"df":1}}},"u":{"docs":{},"df":0,"r":{"docs":{},"df":0,"c":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.4142135623730951}},"df":2}}}},"u":{"docs":{},"df":0,"l":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":2.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":2.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":2.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":2.0}},"df":15}}}},"t":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.4142135623730951}},"df":1}}},"h":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"k":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":2.0}},"df":1}}}},"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"e":{"docs":{},"df":0,"v":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":2.0}},"df":1}}}}},"v":{"docs":{},"df":0,"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":4}}},"i":{"docs":{},"df":0,"e":{"docs":{},"df":0,"w":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951}},"df":1}}},"o":{"docs":{},"df":0,"l":{"docs":{},"df":0,"u":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":3}}}}}}}}},"i":{"docs":{},"df":0,"c":{"docs":{},"df":0,"h":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":1}}}},"g":{"docs":{},"df":0,"h":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":1}},"o":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":2}}},"s":{"docs":{},"df":0,"k":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":2}}},"l":{"docs":{},"df":0,"h":{"docs":{},"df":0,"f":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":1}}},"n":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":2.6457513110645907}},"df":1}},"o":{"docs":{},"df":0,"a":{"docs":{},"df":0,"d":{"docs":{},"df":0,"m":{"docs":{},"df":0,"a":{"docs":{},"df":0,"p":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1}}}}},"b":{"docs":{},"df":0,"o":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.7320508075688772}},"df":3}},"u":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.4142135623730951}},"df":8}}}},"l":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.4142135623730951}},"df":1}},"o":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":2}},"u":{"docs":{},"df":0,"g":{"docs":{},"df":0,"h":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":1}},"t":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":2.0}},"df":1,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":2.23606797749979}},"df":1,"'":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.7320508075688772}},"df":1}}}}}},"u":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":2,"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":1}}}}}},"s":{"docs":{},"df":0,"4":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":1}}},"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"f":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0}},"df":1}}}},"e":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":3.4641016151377544}},"df":1},"f":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.4142135623730951}},"df":3},"t":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":2.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":2.0}},"df":4}}}},"m":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":1},"p":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":3.1622776601683795}},"df":1}}},"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"s":{"docs":{},"df":0,"f":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":1}}}}}}}},"c":{"docs":{},"df":0,"a":{"docs":{},"df":0,"f":{"docs":{},"df":0,"f":{"docs":{},"df":0,"o":{"docs":{},"df":0,"l":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1}}}}},"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"b":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":2.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":5}}},"e":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":2.449489742783178},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":2.449489742783178}},"df":5}},"n":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.7320508075688772}},"df":1,"n":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.7320508075688772}},"df":1}}}}},"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"o":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":4}}}},"e":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":1}}},"f":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":2.6457513110645907}},"df":1}},"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":3.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.4142135623730951}},"df":3}}},"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{},"df":0,"c":{"docs":{},"df":0,"h":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1}}}}}},"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"c":{"docs":{},"df":0,"h":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":2}}}},"c":{"docs":{},"df":0,"u":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":1}}},"e":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.4142135623730951}},"df":1,"d":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":2.23606797749979}},"df":1},"n":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":1}},"g":{"docs":{},"df":0,"m":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":2.23606797749979}},"df":1}}}}},"l":{"docs":{},"df":0,"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":2}}},"f":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":2.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":2.8284271247461903},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":2.0}},"df":3}},"m":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":2.23606797749979},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":2.0}},"df":2}}}},"q":{"docs":{},"df":0,"u":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"c":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.4142135623730951}},"df":2}}}}},"r":{"docs":{},"df":0,"v":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1}},"t":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":3,"u":{"docs":{},"df":0,"p":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0}},"df":1}}},"v":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":1},"r":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.4142135623730951}},"df":5}}}},"h":{"docs":{},"df":0,"i":{"docs":{},"df":0,"f":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":3.3166247903554},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":2,"s":{"docs":{},"df":0,"":{"docs":{},"df":0,"v":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1}}}}}}}},"n":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":1}}},"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":3}},"w":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":8,"c":{"docs":{},"df":0,"a":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0}},"df":1}}},"n":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":3}}}},"i":{"docs":{},"df":0,"g":{"docs":{},"df":0,"n":{"docs":{},"df":0,"i":{"docs":{},"df":0,"f":{"docs":{},"df":0,"i":{"docs":{},"df":0,"c":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.4142135623730951}},"df":14,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{},"df":0,"l":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.7320508075688772}},"df":12}}}}}}}}}}},"m":{"docs":{},"df":0,"i":{"docs":{},"df":0,"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.7320508075688772}},"df":2}}}},"p":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":2.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.7320508075688772}},"df":4,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":2}},"i":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":1,"c":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":1},"f":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":1}}}}},"u":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.7320508075688772}},"df":3,"t":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0}},"df":1}}}}}},"n":{"docs":{},"df":0,"g":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.4142135623730951}},"df":3},"u":{"docs":{},"df":0,"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":2}}}}}},"t":{"docs":{},"df":0,"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1}}}},"z":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":3}}},"k":{"docs":{},"df":0,"i":{"docs":{},"df":0,"l":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":2}}}},"l":{"docs":{},"df":0,"i":{"docs":{},"df":0,"g":{"docs":{},"df":0,"h":{"docs":{},"df":0,"t":{"docs":{},"df":0,"l":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":1}}}}}}},"m":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":2}},"r":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":2,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.4142135623730951}},"df":1}}}}}},"n":{"docs":{},"df":0,"i":{"docs":{},"df":0,"p":{"docs":{},"df":0,"p":{"docs":{},"df":0,"e":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1}}}}}},"o":{"docs":{},"df":0,"f":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":1,"w":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":3.1622776601683795},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.7320508075688772}},"df":3}}}}},"l":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":1},"u":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.4142135623730951}},"df":8}}},"m":{"docs":{},"df":0,"e":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":1}}}}},"p":{"docs":{},"df":0,"h":{"docs":{},"df":0,"i":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":6}}}}},"t":{"docs":{},"df":0,"a":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.7320508075688772}},"df":1}},"u":{"docs":{},"df":0,"r":{"docs":{},"df":0,"c":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":1}}}},"p":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":5}},"r":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":1,"i":{"docs":{},"df":0,"f":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":1}}}}}},"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"i":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0}},"df":1}},"f":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.4142135623730951}},"df":5}},"t":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1}}}}},"e":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.4142135623730951}},"df":2}},"n":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1}}},"l":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.4142135623730951}},"df":1}}}},"s":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":3.605551275463989}},"df":1,"'":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.7320508075688772}},"df":1}}},"t":{"docs":{},"df":0,"a":{"docs":{},"df":0,"b":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0}},"df":1}},"g":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.4142135623730951}},"df":3}},"n":{"docs":{},"df":0,"d":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":6}}}}},"r":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":2}},"t":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.7320508075688772}},"df":3},"i":{"docs":{},"df":0,"c":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":2.0}},"df":1},"s":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":1}}}}},"e":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":4.898979485566356}},"df":2}},"p":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.7320508075688772}},"df":11}},"i":{"docs":{},"df":0,"l":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":6}}},"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":1}}},"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{},"df":0,"g":{"docs":{},"df":0,"h":{"docs":{},"df":0,"t":{"docs":{},"df":0,"f":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"w":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":2}}}}}}}}}}},"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"g":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":1,"i":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.7320508075688772}},"df":5}}}}},"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":1,"l":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1}}}}},"n":{"docs":{},"df":0,"g":{"docs":{},"df":0,"t":{"docs":{},"df":0,"h":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":3}}}}},"i":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":1}}},"o":{"docs":{},"df":0,"n":{"docs":{},"df":0,"g":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":4,"l":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":1}}}}},"u":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":1,"u":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":3}}}},"g":{"docs":{},"df":0,"g":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":3}}}}},"u":{"docs":{},"df":0,"d":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":4}}}},"u":{"docs":{},"df":0,"b":{"docs":{},"df":0,"j":{"docs":{},"df":0,"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":1}}}},"s":{"docs":{},"df":0,"p":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.4142135623730951}},"df":1}}},"t":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":6}}}}}}},"c":{"docs":{},"df":0,"c":{"docs":{},"df":0,"e":{"docs":{},"df":0,"s":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":4,"f":{"docs":{},"df":0,"u":{"docs":{},"df":0,"l":{"docs":{},"df":0,"l":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":1}}}}}}}}},"h":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":4}},"f":{"docs":{},"df":0,"f":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0}},"df":1}},"i":{"docs":{},"df":0,"c":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":1}}}}},"g":{"docs":{},"df":0,"g":{"docs":{},"df":0,"e":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":3}}}}},"i":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":2}},"m":{"docs":{},"df":0,"m":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1}}}},"p":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":6}}}}},"p":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":3}}}},"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":1}}}},"r":{"docs":{},"df":0,"f":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1}}},"p":{"docs":{},"df":0,"a":{"docs":{},"df":0,"s":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":4}}},"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":2,"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"g":{"docs":{},"df":0,"l":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":1}}}}}}}}},"r":{"docs":{},"df":0,"o":{"docs":{},"df":0,"g":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.7320508075688772}},"df":1}}}}},"v":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":2}},"w":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951}},"df":1}},"y":{"docs":{},"df":0,"m":{"docs":{},"df":0,"b":{"docs":{},"df":0,"o":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":2.23606797749979},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":2.6457513110645907}},"df":3}}}},"n":{"docs":{},"df":0,"t":{"docs":{},"df":0,"a":{"docs":{},"df":0,"x":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.4142135623730951}},"df":2}},"h":{"docs":{},"df":0,"e":{"docs":{},"df":0,"s":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":4.0}},"df":1}},"t":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.4142135623730951}},"df":2}}}}},"s":{"docs":{},"df":0,"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":2.6457513110645907},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":2.8284271247461903}},"df":8,"'":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":2.449489742783178}},"df":1}}}}}}},"t":{"docs":{},"df":0,"a":{"docs":{},"df":0,"b":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":1}},"c":{"docs":{},"df":0,"k":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":8}}},"r":{"docs":{},"df":0,"g":{"docs":{},"df":0,"e":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":2.8284271247461903}},"df":1}}}},"s":{"docs":{},"df":0,"k":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":2.449489742783178},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":2.449489742783178},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":2.23606797749979},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":3.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":2.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":2.8284271247461903},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":12}}},"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"h":{"docs":{},"df":0,"n":{"docs":{},"df":0,"i":{"docs":{},"df":0,"q":{"docs":{},"df":0,"u":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":2.0}},"df":10}}}}}},"m":{"docs":{},"df":0,"p":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{},"df":0,"u":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.4142135623730951}},"df":1}}}}}}}},"n":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":1,"s":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":1}},"s":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":2.449489742783178},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":11}},"x":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":3}}},"h":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{},"df":0,"'":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":3}}},"e":{"docs":{},"df":0,"m":{"docs":{},"df":0,"s":{"docs":{},"df":0,"e":{"docs":{},"df":0,"l":{"docs":{},"df":0,"v":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.4142135623730951}},"df":1}}}}},"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.7320508075688772}},"df":2}}}},"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"'":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":2},"f":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0}},"df":1}}}}},"y":{"docs":{},"df":0,"'":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":1}}}},"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"k":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":6}},"r":{"docs":{},"df":0,"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":1}}}}}},"o":{"docs":{},"df":0,"s":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":3}},"u":{"docs":{},"df":0,"g":{"docs":{},"df":0,"h":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":1}}}}},"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":1}},"e":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.4142135623730951}},"df":3}},"o":{"docs":{},"df":0,"u":{"docs":{},"df":0,"g":{"docs":{},"df":0,"h":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.4142135623730951}},"df":1}}}}},"u":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0}},"df":1}},"i":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0}},"df":1,"g":{"docs":{},"df":0,"h":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":1,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.4142135623730951}},"df":1}}}}},"m":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.7320508075688772}},"df":6}},"r":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1}}},"m":{"docs":{},"df":0,"n":{"docs":{},"df":0,"i":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1}}}}},"o":{"docs":{},"df":0,"g":{"docs":{},"df":0,"e":{"docs":{},"df":0,"t":{"docs":{},"df":0,"h":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1}}}},"k":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.7320508075688772}},"df":2}}},"o":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":2.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":7}},"p":{"docs":{"http://127.0.0.1:1111/blog/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":2},"w":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":7}}}},"x":{"docs":{},"df":0,"i":{"docs":{},"df":0,"c":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.7320508075688772}},"df":1}}}},"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"d":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0}},"df":1},"i":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":4}}},"i":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":2.449489742783178},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":2.449489742783178},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":2.449489742783178},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":12,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0}},"df":1}}}},"n":{"docs":{},"df":0,"s":{"docs":{},"df":0,"f":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":4.0}},"df":2}},"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":3.872983346207417},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":4.358898943540674},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":7,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"'":{"docs":{"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":1}}}}}}},"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":4.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":2}}}}}},"e":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.7320508075688772}},"df":3}},"i":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1,"c":{"docs":{},"df":0,"k":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":1}}},"u":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":1},"l":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0}},"df":2}},"m":{"docs":{},"df":0,"p":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":1}},"s":{"docs":{},"df":0,"t":{"docs":{},"df":0,"w":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"t":{"docs":{},"df":0,"h":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1}}}}}}}}}},"t":{"docs":{},"df":0,"a":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":2.6457513110645907}},"df":1}},"u":{"docs":{},"df":0,"n":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":2.449489742783178},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":3.1622776601683795},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":2.0}},"df":4}}},"w":{"docs":{},"df":0,"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"k":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":1}}},"i":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0}},"df":1}},"o":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":6}},"y":{"docs":{},"df":0,"p":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":2.8284271247461903}},"df":7},"i":{"docs":{},"df":0,"c":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1}}}}},"u":{"docs":{},"df":0,"n":{"docs":{},"df":0,"c":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{},"df":0,"i":{"docs":{},"df":0,"b":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.4142135623730951}},"df":1}}}}},"o":{"docs":{},"df":0,"v":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":1}}},"d":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1}},"r":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":3,"l":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":3}},"s":{"docs":{},"df":0,"c":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":1}}},"t":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":2.23606797749979}},"df":6}}}}}}},"o":{"docs":{},"df":0,"u":{"docs":{},"df":0,"b":{"docs":{},"df":0,"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"d":{"docs":{},"df":0,"l":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1}}}}}}}}},"f":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":1}}}},"i":{"docs":{},"df":0,"f":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0}},"df":1}},"o":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1}},"q":{"docs":{},"df":0,"u":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1}}},"l":{"docs":{},"df":0,"i":{"docs":{},"df":0,"k":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":2}},"o":{"docs":{},"df":0,"c":{"docs":{},"df":0,"k":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.4142135623730951}},"df":2}}}},"s":{"docs":{},"df":0,"e":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":5}}}},"t":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"g":{"docs":{},"df":0,"e":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":1}}}}},"r":{"docs":{},"df":0,"u":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":1}}}},"p":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":4,"d":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":2.23606797749979}},"df":1}}}},"r":{"docs":{},"df":0,"d":{"docs":{},"df":0,"f":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.7320508075688772}},"df":1}}},"s":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":2.449489742783178},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":2.449489742783178},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":2.449489742783178},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":2.23606797749979},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":2.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":2.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":3.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":2.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.4142135623730951}},"df":13,"a":{"docs":{},"df":0,"g":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.4142135623730951}},"df":1}},"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1}}},"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":3}}}},"v":{"docs":{},"df":0,"0":{"docs":{},"df":0,".":{"docs":{},"df":0,"2":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0}},"df":1}}},"2":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":1},"a":{"docs":{},"df":0,"l":{"docs":{},"df":0,"i":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":2.0}},"df":4}},"u":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":2,"a":{"docs":{},"df":0,"b":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":2}}}}},"n":{"docs":{},"df":0,"i":{"docs":{},"df":0,"l":{"docs":{},"df":0,"l":{"docs":{},"df":0,"a":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0}},"df":1}}},"s":{"docs":{},"df":0,"h":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":1}}}},"r":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":3,"a":{"docs":{},"df":0,"b":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":2}},"t":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":1}},"e":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":1}}},"o":{"docs":{},"df":0,"u":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":8}}}}},"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":3.4641016151377544}},"df":1}}}},"r":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.4142135623730951}},"df":1,"f":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":3.3166247903554}},"df":1,"i":{"docs":{"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":2.23606797749979}},"df":2}}},"s":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0}},"df":2}}}},"i":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1}}}}}},"i":{"docs":{},"df":0,"a":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":3},"d":{"docs":{},"df":0,"e":{"docs":{},"df":0,"o":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951}},"df":1}}},"e":{"docs":{},"df":0,"w":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951}},"df":1}},"o":{"docs":{},"df":0,"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1}}}},"s":{"docs":{},"df":0,"i":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":2}}},"u":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":2}}}},"v":{"docs":{},"df":0,"i":{"docs":{},"df":0,"d":{"docs":{},"df":0,"l":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":1}}}}}},"l":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":2.23606797749979},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":3.3166247903554}},"df":2,"+":{"docs":{},"df":0,"l":{"docs":{},"df":0,"l":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":2.23606797749979}},"df":1}}}}}}},"w":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":2}}}}},"s":{"docs":{},"df":0,"n":{"docs":{},"df":0,"'":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0}},"df":1}}}},"y":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":11}},"e":{"docs":{},"df":0,"i":{"docs":{},"df":0,"g":{"docs":{},"df":0,"h":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":1}}}},"l":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.4142135623730951}},"df":5}}},"h":{"docs":{},"df":0,"e":{"docs":{},"df":0,"t":{"docs":{},"df":0,"h":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0}},"df":1}}}}}},"i":{"docs":{},"df":0,"d":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1,"r":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":3}}},"n":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":1},"s":{"docs":{},"df":0,"d":{"docs":{},"df":0,"o":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":1}}},"e":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":2}},"t":{"docs":{},"df":0,"h":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":4}},"o":{"docs":{},"df":0,"u":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":2.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":7}}}}}},"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.7320508075688772}},"df":1},"k":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":2.0},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.4142135623730951}},"df":10},"l":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.7320508075688772},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.7320508075688772}},"df":5}}}}},"y":{"docs":{},"df":0,"i":{"docs":{},"df":0,"e":{"docs":{},"df":0,"l":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":3}}}}}}},"title":{"root":{"docs":{},"df":0,"a":{"docs":{},"df":0,"b":{"docs":{},"df":0,"i":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0}},"df":1}}},"c":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"v":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":1}}}},"d":{"docs":{},"df":0,"a":{"docs":{},"df":0,"p":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0}},"df":1}}},"d":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":1}}}},"i":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1},"l":{"docs":{},"df":0,"g":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{},"df":0,"h":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0}},"df":1}}}}}}}},"n":{"docs":{},"df":0,"y":{"docs":{},"df":0,"t":{"docs":{},"df":0,"h":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0}},"df":1}}}},"r":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"c":{"docs":{},"df":0,"u":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951}},"df":1}}}}}},"t":{"docs":{},"df":0,"t":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"k":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":1}}}}},"u":{"docs":{},"df":0,"t":{"docs":{},"df":0,"o":{"docs":{},"df":0,"m":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":2}}}}}},"w":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":1}}}},"b":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":1}}}}},"s":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":1}}},"o":{"docs":{},"df":0,"o":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0}},"df":1}}},"u":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":1}}}}},"c":{"docs":{},"df":0,"a":{"docs":{},"df":0,"u":{"docs":{},"df":0,"s":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":1}}}}},"h":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1}}}},"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"s":{"docs":{},"df":0,"s":{"docs":{},"df":0,"i":{"docs":{},"df":0,"f":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":1}}}}},"u":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1}}}}}},"o":{"docs":{},"df":0,"d":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1}},"m":{"docs":{},"df":0,"p":{"docs":{},"df":0,"i":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":1}}}},"n":{"docs":{},"df":0,"c":{"docs":{},"df":0,"e":{"docs":{},"df":0,"p":{"docs":{},"df":0,"t":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":1}}}}}},"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"u":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0}},"df":1}}},"r":{"docs":{},"df":0,"o":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":3}}}}},"o":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1}}}},"d":{"docs":{},"df":0,"e":{"docs":{},"df":0,"e":{"docs":{},"df":0,"p":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1}},"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"v":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":1}}},"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":1}}}}},"i":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0}},"df":1}}}}},"o":{"docs":{},"df":0,"m":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1}}}}}},"e":{"docs":{},"df":0,"f":{"docs":{},"df":0,"f":{"docs":{},"df":0,"i":{"docs":{},"df":0,"c":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":2}}}}},"n":{"docs":{},"df":0,"g":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":2}}},"h":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"c":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1}}}}}},"f":{"docs":{},"df":0,"a":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":1}}},"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{},"df":0,"u":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":1}}}},"d":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1}}},"e":{"docs":{},"df":0,"d":{"docs":{},"df":0,"b":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{},"df":0,"k":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1}}}}}}},"o":{"docs":{},"df":0,"u":{"docs":{},"df":0,"n":{"docs":{},"df":0,"d":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0}},"df":1}}}}}},"u":{"docs":{},"df":0,"n":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":1}}}}}}}},"g":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0}},"df":1}}}},"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"p":{"docs":{},"df":0,"h":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1}}}}},"i":{"docs":{},"df":0,"m":{"docs":{},"df":0,"a":{"docs":{},"df":0,"g":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":1}},"p":{"docs":{},"df":0,"r":{"docs":{},"df":0,"o":{"docs":{},"df":0,"v":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":2}}}}},"n":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":1}}},"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"p":{"docs":{},"df":0,"r":{"docs":{},"df":0,"e":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":1}}}}}}},"v":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0}},"df":1}}}}},"s":{"docs":{},"df":0,"o":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1}}}},"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"g":{"docs":{},"df":0,"u":{"docs":{},"df":0,"a":{"docs":{},"df":0,"g":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":5}}}}},"r":{"docs":{},"df":0,"g":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":4}}},"e":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1}}},"v":{"docs":{},"df":0,"e":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1}}}},"l":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":3,"s":{"docs":{},"df":0,"c":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":1}}}}}},"o":{"docs":{},"df":0,"g":{"docs":{},"df":0,"i":{"docs":{},"df":0,"c":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1}}},"r":{"docs":{},"df":0,"a":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":1}},"s":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0}},"df":1}},"w":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0}},"df":1}}},"m":{"docs":{},"df":0,"i":{"docs":{},"df":0,"l":{"docs":{},"df":0,"l":{"docs":{},"df":0,"i":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0}},"df":1}}}}},"s":{"docs":{},"df":0,"b":{"docs":{},"df":0,"e":{"docs":{},"df":0,"h":{"docs":{},"df":0,"a":{"docs":{},"df":0,"v":{"docs":{},"df":0,"i":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":1}}}}}}}}}},"o":{"docs":{},"df":0,"d":{"docs":{},"df":0,"e":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.4142135623730951},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"tf":1.0},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":5}}}}},"n":{"docs":{},"df":0,"e":{"docs":{},"df":0,"t":{"docs":{},"df":0,"w":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"k":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1}}}}},"u":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":2}}}}}},"o":{"docs":{},"df":0,"b":{"docs":{},"df":0,"j":{"docs":{},"df":0,"e":{"docs":{},"df":0,"c":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":2}}}}},"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1}}}}}},"p":{"docs":{},"df":0,"i":{"docs":{},"df":0,"c":{"docs":{},"df":0,"k":{"docs":{"http://127.0.0.1:1111/blog/":{"tf":1.0}},"df":1}}},"r":{"docs":{},"df":0,"o":{"docs":{},"df":0,"g":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1}}}},"p":{"docs":{},"df":0,"a":{"docs":{},"df":0,"g":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":1}}},"v":{"docs":{},"df":0,"a":{"docs":{},"df":0,"b":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":1}}}}}}},"q":{"docs":{},"df":0,"u":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{},"df":0,"l":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":1}}}}},"n":{"docs":{},"df":0,"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"z":{"docs":{"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"tf":1.0}},"df":1}}}}}}},"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"k":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0}},"df":1}},"t":{"docs":{},"df":0,"i":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0}},"df":1}}}}}}},"e":{"docs":{},"df":0,"g":{"docs":{},"df":0,"u":{"docs":{},"df":0,"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0}},"df":1}}}}},"l":{"docs":{},"df":0,"i":{"docs":{},"df":0,"a":{"docs":{},"df":0,"b":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1}}}}},"p":{"docs":{},"df":0,"o":{"docs":{},"df":0,"g":{"docs":{},"df":0,"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"p":{"docs":{},"df":0,"h":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1}}}}},"s":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{},"df":0,"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1}}}}}}}},"t":{"docs":{},"df":0,"h":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{},"df":0,"k":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":1}}}}}}},"s":{"docs":{},"df":0,"4":{"docs":{},"df":0,"s":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":1}}},"a":{"docs":{},"df":0,"m":{"docs":{},"df":0,"p":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1}}}},"c":{"docs":{},"df":0,"a":{"docs":{},"df":0,"l":{"docs":{},"df":0,"e":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":1}},"n":{"docs":{"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"tf":1.0}},"df":1}}},"e":{"docs":{},"df":0,"g":{"docs":{},"df":0,"m":{"docs":{},"df":0,"e":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1}}}}},"l":{"docs":{},"df":0,"f":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":1}},"m":{"docs":{},"df":0,"a":{"docs":{},"df":0,"n":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"tf":1.0}},"df":1}}}}},"i":{"docs":{},"df":0,"m":{"docs":{},"df":0,"p":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":1}}}},"k":{"docs":{},"df":0,"i":{"docs":{},"df":0,"l":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0}},"df":1}}}},"o":{"docs":{},"df":0,"f":{"docs":{},"df":0,"t":{"docs":{},"df":0,"w":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"tf":1.0}},"df":1}}}}}},"t":{"docs":{},"df":0,"e":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":1}}},"r":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{},"df":0,"g":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":1}}}}},"u":{"docs":{},"df":0,"b":{"docs":{},"df":0,"s":{"docs":{},"df":0,"p":{"docs":{},"df":0,"a":{"docs":{},"df":0,"c":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0}},"df":1}}}}}},"y":{"docs":{},"df":0,"m":{"docs":{},"df":0,"b":{"docs":{},"df":0,"o":{"docs":{},"df":0,"l":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":1}}}},"n":{"docs":{},"df":0,"t":{"docs":{},"df":0,"h":{"docs":{},"df":0,"e":{"docs":{},"df":0,"s":{"docs":{},"df":0,"i":{"docs":{"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"tf":1.0}},"df":1}}}}}}}},"t":{"docs":{},"df":0,"a":{"docs":{},"df":0,"r":{"docs":{},"df":0,"g":{"docs":{},"df":0,"e":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0}},"df":1}}}}},"h":{"docs":{},"df":0,"e":{"docs":{},"df":0,"m":{"docs":{},"df":0,"s":{"docs":{},"df":0,"e":{"docs":{},"df":0,"l":{"docs":{},"df":0,"v":{"docs":{"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"tf":1.0}},"df":1}}}}}}},"o":{"docs":{},"df":0,"p":{"docs":{"http://127.0.0.1:1111/blog/":{"tf":1.0}},"df":1}},"r":{"docs":{},"df":0,"a":{"docs":{},"df":0,"i":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"tf":1.0}},"df":1}},"n":{"docs":{},"df":0,"s":{"docs":{},"df":0,"f":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.4142135623730951}},"df":1}},"o":{"docs":{},"df":0,"r":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"tf":1.0},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"tf":1.0}},"df":2}}}},"l":{"docs":{},"df":0,"a":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0}},"df":1}}}}}}}},"u":{"docs":{},"df":0,"s":{"docs":{"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"tf":1.0}},"df":1}},"v":{"docs":{},"df":0,"e":{"docs":{},"df":0,"r":{"docs":{},"df":0,"i":{"docs":{},"df":0,"f":{"docs":{"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"tf":1.0}},"df":1}}}},"i":{"docs":{},"df":0,"a":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0}},"df":2},"s":{"docs":{},"df":0,"i":{"docs":{},"df":0,"o":{"docs":{},"df":0,"n":{"docs":{"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"tf":1.0}},"df":1}}}}},"l":{"docs":{},"df":0,"m":{"docs":{"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"tf":1.0}},"df":1}}},"w":{"docs":{},"df":0,"i":{"docs":{},"df":0,"t":{"docs":{},"df":0,"h":{"docs":{},"df":0,"o":{"docs":{},"df":0,"u":{"docs":{},"df":0,"t":{"docs":{"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"tf":1.0}},"df":1}}}}}}}}}},"documentStore":{"save":true,"docs":{"http://127.0.0.1:1111/":{"body":"","id":"http://127.0.0.1:1111/","title":""},"http://127.0.0.1:1111/blog/":{"body":"","id":"http://127.0.0.1:1111/blog/","title":"All of our top picks"},"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"body":"Automating Articulated 3D Object Modeling\nThis blog post discusses a research paper that introduces Articulate-Anything, a groundbreaking system for automatically generating articulated 3D models. This system is a game-changer for fields like AR/VR, animation, and robotics, where creating realistic 3D models is often a time-consuming and expert-driven process.\nWhat is Articulate-Anything?\nArticulate-Anything is a system that automates the creation of articulated 3D objects from diverse inputs such as text, images, and videos.  It uses vision-language models (VLMs) to generate Python code, which is then compiled into URDF (Unified Robot Description Format) files. These URDF files are compatible with standard 3D simulators, allowing for the creation of interactive digital twins.  The system's clever approach uses an actor-critic method: a VLM actor proposes solutions, and a VLM critic evaluates and refines them iteratively, leading to highly accurate and realistic models.\nHow Does it Work?\nThe magic behind Articulate-Anything lies in its three-stage process:\n\n\nMesh Retrieval: The system starts by retrieving 3D meshes from a library (like PartNet-Mobility) based on the input.  For visual inputs, it uses CLIP similarity for efficient retrieval.  Text inputs are processed by an LLM to predict object parts and their dimensions.\n\n\nLink Placement:  An actor-critic VLM system iteratively places the retrieved meshes in 3D space.  The actor generates Python code for placement, while the critic assesses the result based on visual similarity to the input and provides feedback for refinement.\n\n\nJoint Prediction: Another actor-critic system predicts the kinematic joints between the placed meshes. Similar to link placement, the actor generates Python code, and the critic evaluates and refines based on video input (if available) to ensure accurate joint movement.\n\n\nThe generated Python code is then compiled into the URDF format, making the models ready for use in any standard 3D simulator.\nImpressive Results\nArticulate-Anything significantly surpasses existing methods.  On the PartNet-Mobility dataset, it boasts a remarkable 75% success rate, a huge leap from the 8.7-12.2% achieved by previous techniques.  The system's ability to handle complex objects and ambiguous affordances from various input types is truly impressive.  Furthermore, the generated models have proven useful for training advanced robotic manipulation policies.\nChallenges and Future Directions\nWhile Articulate-Anything is a significant breakthrough, there are still some challenges to address:\n\n\nData Dependency: The system's performance heavily relies on the quality and completeness of the underlying 3D asset dataset.  Improving the system's generalization to unseen objects requires addressing this dependency.\n\n\nComputational Cost:  Training and running the VLMs demand substantial computational resources.\n\n\nComplexity: The system's architecture is complex, which could make maintenance and expansion difficult.\n\n\nGeneralization:  Further testing on diverse datasets is crucial to fully assess its generalization capabilities.\n\n\nFuture research could explore several exciting avenues:\n\n\nImproved Mesh Generation: Integrating more advanced mesh generation models could enhance the quality and realism of the generated assets.\n\n\nMore Diverse Datasets:  Expanding the datasets used for evaluation would provide a more comprehensive understanding of the system's capabilities.\n\n\nEnhanced Critic Design:  More sophisticated critic models could further improve accuracy and efficiency.\n\n\nReal-time Articulation:  Optimizing the system for real-time articulation generation would open up exciting interactive applications.\n\n\nConclusion\nArticulate-Anything is a significant step forward in automatic articulated object modeling. Its accuracy, robustness, and versatility across various input modalities showcase its potential to revolutionize many fields.  Addressing the current challenges and exploring the future directions mentioned above will pave the way for even more impressive advancements in the future.\n","id":"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/","title":"Articulate-Anything: Automatic Modeling of Articulated Objects via a Vision-Language Foundation Model"},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"body":"Boosting LLM Translation Without Losing Smarts\nIntroduction: The Problem of Catastrophic Forgetting\nLarge Language Models (LLMs) are amazing, excelling at many tasks. But when it comes to machine translation (MT), traditional fine-tuning methods often cause a frustrating problem: catastrophic forgetting.  The LLM gets better at translating, but loses some of its general knowledge and ability to perform other tasks.  This is a big deal, especially for safety and instruction-following capabilities, which are often developed with proprietary data.\nThis research paper introduces a clever solution called RaDis (Rationale Distillation) that tackles this head-on.  Instead of just training the LLM on translations, RaDis uses the LLM's own ability to generate rationales  basically explanations  for its translations. These rationales act as a form of \"replay data,\" helping the LLM retain its general knowledge while learning new translation skills.\nRaDis: A Novel Approach to LLM Fine-tuning\nThe core of RaDis is simple yet effective:\n\n\nLLM Rationale Generation: The researchers observed that instruction-tuned LLMs often generate detailed rationales when asked to translate. These rationales contain general knowledge and safety principles.\n\n\nEnriched Training Data: RaDis uses the LLM to generate these rationales for the training data's reference translations.  These rationales are then combined with the reference translations to create an enriched dataset.\n\n\nCombined Loss Function: The model is trained on this enriched dataset using a loss function that includes both a standard translation loss and a self-distillation loss on the rationale tokens. This self-distillation encourages the model to retain the knowledge encapsulated in the rationales.\n\n\nExperiment Results:  Success Without Forgetting\nThe researchers tested RaDis on two popular LLMs: LLaMA-2-7B-Chat and Mistral-7B-Instruct-v0.2. The results were impressive:\n\n\nSignificant Improvement in Translation: RaDis substantially improved translation performance compared to various baseline methods, including vanilla fine-tuning and other continual learning techniques.\n\n\nPreservation of General Abilities: Importantly, RaDis did not cause a significant drop in the LLMs' performance on general ability benchmarks like MT-Bench, AlpacaEval, safety, and reasoning tasks. This confirms that RaDis effectively addresses catastrophic forgetting.\n\n\nAblation Study:  The researchers conducted an ablation study to confirm the importance of using self-generated rationales  showing that using rationales from other LLMs wasn't as effective.\n\n\nRaDis:  Strengths, Limitations, and Future Directions\nStrengths:\n\nEffectively tackles catastrophic forgetting in LLM fine-tuning for MT.\nSignificantly improves translation performance.\nPreserves general LLM capabilities.\nRelatively straightforward to implement.\n\nLimitations:\n\nComputational Cost: Generating rationales for every training example can be computationally expensive.\nRationale Quality: The effectiveness depends on the quality of the self-generated rationales.\nGeneralizability: Further research is needed to confirm how well RaDis generalizes to other LLMs and tasks.\nBias: The rationales (and therefore the translation model) might inherit biases from the training data.\n\nFuture Directions:\n\nApplying RaDis to other NLP tasks.\nResearch into improving the quality of generated rationales.\nDeveloping more efficient ways to generate and use rationales.\nInvestigating RaDis's performance on different LLM architectures.\nIncorporating human feedback to improve rationale quality.\n\nConclusion: A Promising Step Forward\nRaDis presents a promising new approach to fine-tuning LLMs for translation. By cleverly leveraging the LLMs' own ability to generate rationales, RaDis offers a path towards creating more versatile and robust LLMs that excel in specialized tasks without sacrificing their overall intelligence and safety.  The research opens exciting avenues for future work in mitigating catastrophic forgetting and building more capable and reliable LLMs.\n","id":"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/","title":"Boosting LLM Translation Skills without General Ability Loss via Rationale Distillation"},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"body":"Controlled Low-Rank Adaptation for LLMs\nIntroduction: Tackling Catastrophic Forgetting in LLMs\nLarge language models (LLMs) are incredibly powerful, but they suffer from a significant drawback: catastrophic forgetting.  This means that when you train an LLM on a new task, it can lose its ability to perform well on previously learned tasks. This paper introduces Controlled LoRA (CLoRA), a new method designed to significantly reduce this forgetting while maintaining efficiency.\nThe Problem: Catastrophic Forgetting in Continued LLM Training\nCatastrophic forgetting is a major hurdle in the continued training of LLMs.  Existing methods like Low-Rank Adaptation (LoRA) improve parameter efficiency but don't fully address the forgetting problem.  They constrain the rank of the update matrix but not its direction, which significantly influences how the LLM changes its output.\nCLoRA: A Novel Subspace Regularization Approach\nCLoRA cleverly addresses this limitation by adding a constraint on the direction of updates within the null space of the updating matrix.  This constraint works by limiting the scale of output changes caused by updates, thus mitigating catastrophic forgetting without severely limiting model capacity.  Think of it as guiding the LLM's learning in a more controlled manner.\nThe core idea is illustrated in the paper's Figure 1. By constraining the direction of updates within the null space, CLoRA ensures that the changes to the model's output are more focused and less likely to disrupt previously learned knowledge.  This approach elegantly balances model capacity and the risk of forgetting.\nMethodology: Controlled Experiments and Benchmark Datasets\nThe researchers used a rigorous experimental setup to evaluate CLoRA's performance. They compared it against several baseline methods, including standard LoRA, DORA, PiSSA, MiLoRA, and methods using rank reduction or L2 regularization.  The experiments involved various benchmark datasets, including those focusing on commonsense and mathematical reasoning, allowing for both in-domain and out-of-domain evaluations.  They measured performance using accuracy scores.\nImportantly, they investigated the impact of different initialization methods for CLoRA's regularization matrices, exploring random initialization and SVD-based approaches.  The optimal size (k) of the regularization matrix is shown to be task-dependent.\nResults: Superior Performance and Capacity-Forgetting Balance\nCLoRA consistently outperformed existing LoRA-based methods on both in-domain and out-of-domain evaluations. This clearly demonstrates its effectiveness in mitigating catastrophic forgetting.  Analysis of model parameters confirmed that CLoRA effectively balances the trade-off between model capacity and forgetting, achieving a reduction in output changes without significantly compromising capacity.\nConclusion: A Promising Approach for Continued LLM Training\nCLoRA presents a significant advancement in parameter-efficient fine-tuning for LLMs. Its superior performance across various benchmarks and its ability to balance model capacity and forgetting make it a promising technique for enabling the continued training of LLMs without the pitfalls of catastrophic forgetting.\nFuture Work and Potential Improvements\nWhile highly effective, there's always room for improvement.  Future work could explore:\n\nAlternative regularization strategies:  Exploring regularization methods beyond orthogonal regularization.\nAdaptive k selection: Developing a more sophisticated method for choosing the optimal k value based on the task and dataset.\nBroader LLM architecture testing: Evaluating CLoRA's performance on a wider range of LLM architectures.\nMore complex continual learning scenarios: Applying CLoRA to scenarios with more frequent task changes or a larger number of tasks.\nDeeper theoretical analysis: Further theoretical analysis to better understand CLoRA's mechanism.\n\n","id":"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/","title":"Controlled Low-Rank Adaptation with Subspace Regularization for Continued Training on Large Language Models"},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"body":"COOL Program Synthesis:  Efficiency &amp; Reliability\nIntroduction: Revolutionizing Program Synthesis with COOL\nProgram synthesis, the automated creation of computer programs, is rapidly evolving.  While existing methods have shown promise, they often fall short when dealing with complex tasks.  They frequently lack the fine-grained control and flexible modularity needed for real-world software development.  This is where COOL steps in.\nCOOL (Chain-Oriented Objective Logic with Neural Network Feedback Control) is a groundbreaking neural-symbolic framework designed to overcome these limitations.  It achieves this through two key innovations:\n\n\nChain-of-Logic (CoL): CoL meticulously organizes the program synthesis process into a structured sequence of stages, providing precise control and improving interpretability.  Think of it as a carefully planned roadmap guiding the synthesis process towards the desired outcome.\n\n\nNeural Network Feedback Control (NNFC): NNFC integrates neural networks to dynamically refine the synthesis process, correcting errors and adapting to changing conditions in real-time. It's like having a skilled assistant constantly monitoring and adjusting the synthesis process for optimal performance.\n\n\nThis blog post will delve into the details of COOL, exploring its methodology, results, potential issues, and future opportunities.\nCOOL's Methodology: A Blend of Symbolic and Neural Approaches\nCOOL's strength lies in its unique blend of symbolic reasoning (CoL) and neural network learning (NNFC).  The researchers rigorously evaluated COOL's performance through both static and dynamic experiments:\n\n\nStatic Experiments:\nThese experiments used pre-trained neural networks and controlled conditions to isolate the impact of CoL.  They compared multiple groups: a baseline DSL, a heuristic-enhanced DSL, and a CoL DSL (with and without neural networks). This allowed for a clear assessment of CoL's contribution to improved accuracy and efficiency.\n\n\nDynamic Experiments:\n\n\nThese experiments evaluated COOL under more realistic conditions with varying task domains and difficulties. Neural networks were trained during the synthesis process, reflecting real-world scenarios. This phase revealed the robustness and adaptability of the NNFC mechanism in handling dynamic challenges.\nPerformance was meticulously measured using several metrics: accuracy, average tree operations, average transformation pairs, average neural network invocations, and average time spent.  Both relational and symbolic program synthesis tasks of varying complexity served as benchmarks.\nCOOL's Impressive Results: A Significant Leap in Program Synthesis\nThe results from both static and dynamic experiments overwhelmingly supported the hypotheses.  CoL significantly improved accuracy and efficiency across the board.  NNFC further enhanced accuracy, particularly in dynamic experiments where adaptability was crucial.\nHere are some key findings:\n\nCoL: In static experiments, CoL improved accuracy by 70%, reduced tree operations by 91%, and decreased time by 95%.\nNNFC: In dynamic experiments, NNFC boosted accuracy by an additional 6% and reduced tree operations by 64% under challenging conditions.\n\nThese results highlight COOL's potential to revolutionize program synthesis, offering a highly efficient and reliable framework for complex tasks.\nAddressing Potential Issues and Exploring Future Opportunities\nWhile COOL demonstrates significant improvements, addressing certain challenges is crucial for broader adoption:\nPotential Issues:\n\nScalability:  Further investigation into COOL's scalability to handle extremely large and complex programs is needed.\nGeneralizability:  More extensive testing is required to determine COOL's generalizability across different programming languages and problem domains.\nComputational Cost: The integration of neural networks increases computational cost, necessitating optimization strategies.\nExplainability: While CoL enhances interpretability, improving the explainability of NNFC remains an important goal.\n\nFuture Opportunities:\n\nRefinement of CoL and NNFC:  Further research could explore more sophisticated control mechanisms within CoL and advanced learning strategies for NNFC.\nExpanding Application Domains:  Applying COOL to diverse program synthesis domains, such as natural language processing and robotics, is a promising avenue for exploration.\nIntegration with LLMs:  Integrating COOL with Large Language Models (LLMs) could potentially unlock even greater performance.\nImproved Explainability:  Research into techniques to improve the explainability of NNFC would greatly enhance trustworthiness and user adoption.\nAddressing Scalability Challenges:  Developing strategies to enhance COOL's scalability is crucial for real-world applications.\n\nConclusion: COOL's Promise for the Future of Program Synthesis\nCOOL represents a significant advancement in program synthesis. Its combination of structured symbolic reasoning (CoL) and adaptive neural network control (NNFC) provides a highly efficient and reliable framework for tackling complex and dynamic program synthesis tasks. While challenges remain, the potential of COOL to transform software development is undeniable.  Further research and development in the areas outlined above will undoubtedly lead to even more impactful advancements in this exciting field.\n","id":"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/","title":"COOL: Efficient and Reliable Chain-Oriented Objective Logic with Neural Networks Feedback Control for Program Synthesis"},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"body":"Sample Clustered Federated Learning for Semantic Segmentation\nIntroduction: Tackling Covariate Shifts in Federated Learning\nFederated learning (FL) has emerged as a powerful tool for training models on decentralized datasets without compromising data privacy. However, FL faces challenges when dealing with Non-Independent and Identically Distributed (Non-IID) data across participating clients. While previous research focused on label or concept shifts, this paper dives into the impact of covariate shiftsvariations in data distributionson the performance of federated learning for 2D semantic segmentation.\nWe find that covariate shifts, although less impactful than label shifts, still hinder convergence in FL for semantic segmentation.  To address this, we propose a novel framework called Sample Clustered Federated Learning (SCFL). This innovative approach significantly improves convergence speed and generalization performance compared to existing techniques.\nUnderstanding Covariate Shifts and Existing Limitations\nCovariate shifts represent differences in the underlying feature distributions of data across clients. Unlike label shifts (differences in class distributions) or concept shifts (differences in task definitions), covariate shifts affect the model's ability to generalize to unseen data.  Existing personalized and clustered federated learning methods often fall short in scenarios where each client's data encompasses multiple underlying feature domains. These methods typically operate at the client level, assuming homogeneity within each client's dataset. This assumption is frequently violated in real-world applications.\nThe Sample Clustered Federated Learning (SCFL) Framework\nSCFL introduces a sample-level clustering approach to overcome these limitations.  It leverages a novel technique called Deep Domain Isolation (DDI) to identify and isolate image domains directly in the model's gradient space. This is achieved using a Federated Gaussian Mixture Model (Fed-GMM) and spectral clustering.  The key steps of SCFL are:\n\nFederated Pretraining:  Initializes a global model using FedAvg.\nClustering using DDI: Isolates image domains at the sample level using gradient analysis. A pruned version of DDI is also introduced to mitigate computational costs.\nSample Clustered Federated Refinement: Trains independent models for each isolated domain using FedAvg.\nTest-Time Cluster Assignment: Assigns test samples to their corresponding domains using a trained classifier, eliminating the need for assumptions about the test data distribution.\n\nExperimental Results and Validation\nExperiments were conducted on both synthetic (TMNIST-Inv) and real-world (Cityscapes+GTA5) datasets with various data splits (IID, Full non-IID, Dirichlet non-IID) to simulate different levels of data heterogeneity.  The results demonstrate:\n\nCovariate shifts negatively impact the convergence of standard FedAvg, especially in Full non-IID settings.\nSCFL significantly outperforms all baseline methods (FedAvg, SCAFFOLD, personalized FedAvg+, and CFL) across all data splits and datasets, particularly in Non-IID scenarios.  This improvement is measured by a substantial increase in mean Intersection over Union (mIoU).\nDDI effectively isolates image domains, exhibiting high clustering accuracy.\nThe test-time cluster assignment mechanism accurately assigns test samples to the correct domains.\nPruned DDI maintains high performance even with significant gradient pruning.\n\nConclusion and Future Directions\nSCFL presents a robust framework for handling covariate shifts in federated learning for semantic segmentation. Its sample-level approach improves convergence, generalization, and offers a solution that is agnostic to test distribution assumptions.\nFuture research could explore alternative clustering techniques, investigate the impact of different model architectures, extend SCFL to handle multiple types of Non-IID data, and apply it to other computer vision tasks.  Further investigation into computational efficiency and scalability is also warranted.\n","id":"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/","title":"Deep Domain Isolation and Sample Clustered Federated Learning for Semantic Segmentation"},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"body":"Unlocking LLMs: Automating the Interpretation of Millions of Features\nIntroduction:  Peeking Inside the Black Box\nLarge language models (LLMs) are amazing, but understanding how they work remains a significant challenge.  While individual neurons offer limited insight, sparse autoencoders (SAEs) provide a promising pathway to unlock their inner workings. SAEs transform the complex LLM activations into a higher-dimensional latent space  potentially easier for humans to interpret. However, these SAEs can have millions of latent features, making manual interpretation impossible. This research introduces an automated framework to tackle this challenge head-on.\nThe Automated Interpretation Pipeline: From Latent Features to Human-Understandable Explanations\nThis research paper details a novel, open-source framework designed to automatically generate and evaluate natural language explanations for the latent features learned by SAEs trained on LLMs. This innovative pipeline consists of several key steps:\n\nSAE Training:  SAEs are trained on different LLMs (Llama, Gemma) using varied architectures, activation functions, and loss functions.\nActivation Collection: Latent activations are collected from the trained SAEs using a large text corpus (RedPajama-v2, Pile).\nExplanation Generation: LLMs (Llama 70b, Claude) generate natural language explanations for each latent feature, based on its activation patterns.  Clever prompting techniques are used to maximize the quality and clarity of these explanations.\nExplanation Scoring: Five novel, computationally efficient scoring methods (Detection, Fuzzing, Surprisal, Embedding, Intervention) are introduced to assess explanation quality. These methods are compared to existing, more computationally expensive simulation-based methods. The new methods focus on the ability of the explanations to distinguish between activating and non-activating contexts.\nSemantic Similarity Analysis:  The semantic similarity between independently trained SAEs across different LLM layers is measured using the Hungarian algorithm to align latent features and comparing their explanations.\n\nKey Findings:  SAE Latents Outperform Individual Neurons\nThe research yields several key findings:\n\nSuperior Interpretability: SAE latents prove considerably more interpretable than individual neurons, even when neurons are sparsified using top-k selection.\nEfficient Automated Explanations: The LLM-based framework successfully generates millions of high-quality explanations.\nEffective Novel Scoring Methods: The new scoring methods show strong correlations with simulation-based methods, offering a more computationally efficient approach.  Intervention scoring, in particular, reveals previously hidden features.\nLayer-Wise Semantic Consistency: SAEs trained on adjacent layers of the residual stream exhibit high semantic similarity, revealing a degree of consistency in feature representation.\n\nFuture Directions and Limitations\nWhile the results are promising, certain limitations and potential areas for improvement exist:\n\nBias in Scoring: The reliance on specific datasets and prompts for evaluation could introduce biases into the explanation scoring.\nComputational Resources:  While more efficient than simulation methods, generating and evaluating millions of explanations still requires significant computational resources.\nGeneralizability: The findings might not perfectly generalize to all LLMs, SAE architectures, or datasets.\nSubjectivity in Quality Assessment: Incorporating human evaluation of explanation quality could provide a more complete assessment.\n\nFuture research could focus on:\n\nImproving Explanation Quality: Refining prompting techniques and utilizing more advanced LLMs.\nDeveloping New Scoring Metrics: Exploring additional metrics to capture different aspects of explanation quality.\nExtending to Other Modalities: Applying the framework to other data types beyond text.\nFurther Investigating Intervention Scoring:  Expanding its application to different tasks and LLMs.\nSteering and Concept Localization: Leveraging the generated explanations to improve methods for steering and localizing concepts within LLMs.\n\nThis research represents a substantial step towards a deeper understanding of LLMs. By automating the interpretation of millions of latent features, this work opens exciting new avenues for research and application, paving the way for more interpretable and controllable AI systems.  The authors have made their code and generated explanations publicly available, encouraging further exploration and development in this rapidly evolving field.\n","id":"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/","title":"Automatically Interpreting Millions of Features in Large Language Models"},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"body":"Can LLMs Invent Algorithms to Improve Themselves?\nThis post explores a research paper investigating whether Large Language Models (LLMs) can autonomously create algorithms to enhance their performance.  It's a fascinating look into the future of AI self-improvement!\nThe Self-Developing Framework: LLMs Inventing Their Own Improvements\nThe paper introduces the \"Self-Developing\" framework. This framework lets an LLM (the \"seed model\") iteratively generate, test, and refine model-improvement algorithms. Think of it as an LLM creating its own personal trainer!\nHere's the process:\n\nAlgorithm Factory Initialization: A copy of the seed model is created, acting as an \"algorithm factory.\"\nAlgorithm Generation: The algorithm factory generates Python code for these algorithms.\nAlgorithm Evaluation: The new algorithms are applied to the seed model, and the results are evaluated on benchmark datasets (GSM8k and MATH).\nAlgorithm Factory Refinement:  Direct Preference Optimization (DPO) refines the algorithm factory based on the results.  Essentially, it learns from what works and what doesn't.\nIterative Improvement: Steps 2-4 repeat, simultaneously improving both the seed model and the algorithm factory.\n\nKey Findings: LLMs Outperform Human-Designed Algorithms\nThe results were impressive!  The Self-Developing framework produced LLMs that outperformed the original seed model and even surpassed models improved by human-designed algorithms like Task Arithmetic and TIES merging.  Specific improvements included:\n\nA 6% improvement on the GSM8k dataset.\nStrong transferability of the LLM-discovered algorithms to different, unseen models.\nHigher initial temperatures were beneficial in generating diverse algorithms, while temperature decay led to more stable performance.\n\nChallenges and Future Directions\nWhile exciting, the research also highlighted some challenges:\n\nGeneralizability:  The framework's effectiveness might be limited to certain tasks or LLMs. More research is needed.\nComputational Cost: The iterative process is computationally expensive.\nAlgorithm Interpretability: Understanding exactly how the LLM-generated algorithms work is difficult.\nBias and Safety:  LLMs can inherit biases from their training data, a crucial concern for any self-improving system.\n\nFuture work could explore different optimization methods, apply the framework to other tasks, improve algorithm interpretability, and address bias and safety concerns.  The potential is enormous!\nConclusion: A Glimpse into the Future of AI\nThis research demonstrates that LLMs can autonomously develop effective self-improvement algorithms, exceeding human-designed methods. The Self-Developing framework opens up exciting new possibilities for AI development with minimal human intervention, paving the way for more efficient and autonomous AI systems.  It's a fascinating step towards truly self-improving AI!\n","id":"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/","title":"Can Large Language Models Invent Algorithms to Improve Themselves?"},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"body":"LLM Misbehavior Detection: Introducing LLMSCAN\nIntroduction:  Catching LLMs in the Act\nLarge language models (LLMs) are amazing, but they can sometimes produce outputs that are untrue, biased, toxic, or even harmful  especially when tricked with \"jailbreak\" prompts.  Existing methods for detecting this misbehavior often focus on a single problem or analyze only the LLM's output.  But what if we could look inside the LLM's \"brain\" to catch bad behavior in the act?\nThat's the idea behind LLMSCAN, a new technique detailed in the research paper \"LLMSCAN: Causal Scan for LLM Misbehavior Detection\".  Instead of just examining the final output, LLMSCAN uses causality analysis to monitor the LLM's internal processes.  Think of it as a lie detector for LLMs, but one that analyzes the LLM's internal \"thought process\" rather than just its words.\nHow LLMSCAN Works: A Two-Stage Process\nLLMSCAN works in two stages:\n\n\nScanning: A \"scanner\" uses lightweight causality analysis to create a \"causal map\" of the LLM's internal activity. This involves:\n\nToken-level analysis:  The scanner intervenes on individual input tokens (words or parts of words) to see how they influence the attention scores (how much the model focuses on different parts of the input).\nLayer-level analysis: The scanner intervenes on individual transformer layers (the building blocks of the LLM) to see how they influence the output.\n\n\n\nDetection: A simple classifier (a Multi-layer Perceptron, or MLP) is trained on these causal maps.  It learns to distinguish between causal maps from normal LLM behavior and those indicating misbehavior.  The model uses statistical features (like mean, standard deviation, etc.) from the token-level analysis to create a consistent input for the classifier, regardless of the input length.\n\n\nImpressive Results: High Accuracy Across Multiple LLMs and Datasets\nThe researchers tested LLMSCAN on four popular LLMs and thirteen datasets covering four types of misbehavior:\n\nLie detection: Identifying when the LLM fabricates information.\nJailbreak detection: Detecting attempts to bypass the LLM's safety mechanisms.\nToxicity detection: Identifying abusive or offensive language.\nBias detection: Identifying unfair or discriminatory language.\n\nThe results were very promising! LLMSCAN achieved an average AUC (Area Under the Curve) score above 0.95 for lie detection, jailbreak detection, and toxicity detection  indicating high accuracy. While the performance on bias detection was slightly lower, it still significantly outperformed the baselines (existing methods).  Importantly, LLMSCAN can often detect misbehavior early in the generation process, before the full output is even generated.\nPotential Limitations and Future Directions\nWhile LLMSCAN shows great promise, there are some areas for improvement:\n\nGeneralizability: More testing is needed to ensure its robustness across a wider range of LLMs and unseen data.\nComputational cost: While efficient, the computational cost of causality analysis could be significant for extremely large LLMs or very long inputs.\nInterpretability: The causal maps provide insights, but their interpretation requires further research.\nAdversarial attacks:  Further research is needed to test its robustness against sophisticated adversarial attacks designed to fool the system.\n\nFuture work could focus on expanding the types of misbehavior detected, optimizing the causality analysis for efficiency, using more advanced machine learning models for the detector, and integrating LLMSCAN into real-time LLM monitoring systems.\nConclusion: A Promising New Tool for LLM Safety\nLLMSCAN offers a novel and effective approach to detecting LLM misbehavior. By analyzing the internal workings of the LLM, rather than just its output, it achieves high accuracy and efficiency across various types of misbehavior.  This generalizable approach addresses limitations of existing methods, paving the way for more robust and comprehensive LLM safety mechanisms.  It's a significant step toward making LLMs safer and more reliable.\n","id":"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/","title":"LLMScan: Causal Scan for LLM Misbehavior Detection"},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"body":"Enhancing QLoRA: QuAILoRA's Quantization-Aware Initialization\nIntroduction: Fine-tuning LLMs Efficiently with QuAILoRA\nFine-tuning large language models (LLMs) is computationally expensive.  Quantized Low-Rank Adaptation (QLoRA) offers a clever solution by quantizing the base model, significantly reducing memory usage. However, this quantization introduces errors that can hurt the model's performance.  This is where QuAILoRA steps in!  This research introduces a novel quantization-aware initialization method for QLoRA, designed to minimize these pesky quantization errors right from the start.\nQuAILoRA: A Quantization-Aware Approach\nQuAILoRA cleverly tackles the performance drop caused by QLoRA's quantization. It does this by carefully initializing the LoRA matrices (A and B). The goal? To keep the activations (or weights) of the QLoRA model as close as possible to a full-precision base model during initialization.\nThe method uses a two-step process:\n\nUncalibrated Initialization:  QuAILoRA begins by using Singular Value Decomposition (SVD) to initialize A and B, minimizing an uncalibrated quantization objective. Think of this as a rough first draft.\nCalibrated Refinement:  Next, it iteratively refines these initializations using a calibrated quantization objective function. This refinement ensures the QLoRA model's behavior closely matches the full-precision model, especially on a chosen calibration dataset.  This is where the magic happens, fine-tuning the initial draft for optimal performance.\n\nThis entire process is computationally inexpensive, requiring only the solution of small linear systems.\nExperimental Results: Across LLMs and Tasks\nThe researchers put QuAILoRA to the test across various LLMs (LLaMA, OPT, BLOOM, Pythia), model sizes, and downstream tasks. The results are impressive:\n\nConsistent Improvement: QuAILoRA consistently improves validation perplexity compared to standard QLoRA initialization.\nSignificant Gains with Low-bit Quantization: The benefits are even more pronounced when using lower-bit quantization (e.g., 4-bit), significantly reducing memory footprint.  In fact, QuAILoRA with 4-bit QLoRA models achieved about 75% of the performance improvement seen in 8-bit QLoRA models!\nRobustness to Calibration Dataset: The method is remarkably robust to the choice of calibration dataset.\nEnhanced LoRA Rank Performance:  Increasing the LoRA rank further boosted QuAILoRA's performance, unlike the baseline QLoRA.\nNo Impact on Convergence Speed: QuAILoRA didn't significantly affect the fine-tuning convergence speed.\n\nConclusion: A Practical Leap Forward for Efficient Fine-tuning\nQuAILoRA offers a practical and effective way to enhance QLoRA's performance without increasing memory usage during fine-tuning. It consistently boosts performance across a variety of LLMs and tasks, particularly where quantization errors are more significant. This makes it a valuable tool for researchers and practitioners looking to efficiently fine-tune LLMs.\nFuture Directions: Exploring the Potential\nWhile QuAILoRA shows great promise, there's plenty of room for further exploration:\n\nScaling to Larger Models:  Testing QuAILoRA on even larger LLMs (beyond 30B parameters) could uncover further insights.\nLower Bit-depth Quantization: Exploring extremely low-bit quantization (e.g., 1-bit, 2-bit) could lead to even greater memory efficiency.\nAdaptive Calibration Strategies:  Dynamically adjusting the calibration dataset during fine-tuning could further optimize performance.\nExtending to Other PEFT Methods: The core concepts of QuAILoRA could be beneficial for other parameter-efficient fine-tuning methods.\n\nQuAILoRA represents a significant step towards more efficient and effective fine-tuning of LLMs.  Its simplicity and effectiveness make it a promising technique for future research and applications.\n","id":"http://127.0.0.1:1111/blog/quailora-lora-quantization/","title":"QuAILoRA: Quantization-Aware Initialization for LoRA"},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"body":"RepoGraph: Boosting AI Software Engineering\nIntroduction: Level Up Your AI Software Engineering with RepoGraph\nAre you tired of AI software engineering tools that only scratch the surface?  Many existing tools focus on individual functions or files, missing the crucial context of the entire repository. This is like trying to fix a car engine without understanding how all the parts work together! That's where RepoGraph comes in.\nThis groundbreaking research introduces RepoGraph, a plugin module designed to revolutionize AI software engineering by providing a comprehensive, repository-level understanding of your code.  Imagine having a bird's-eye view of your entire codebase, revealing hidden dependencies and relationships. That's the power of RepoGraph.\nWhat is RepoGraph and Why Does it Matter?\nRepoGraph constructs a repository-wide graph.  Each node represents a single line of code, and edges connect lines that depend on each other.  This graph allows for fine-grained analysis, enabling more effective solutions to complex, repository-level coding problems.\nWhy is this significant?  Because real-world software isn't just a collection of isolated files  it's a complex ecosystem of interconnected components. Existing AI tools often struggle with this complexity, leading to inefficient solutions or missed opportunities. RepoGraph changes the game by providing the holistic view necessary for superior performance.\nHow RepoGraph Works: A Three-Step Process\nRepoGraph's magic lies in its three-step construction process:\n\n\nCode Line Parsing: RepoGraph uses tree-sitter to parse your code, creating an Abstract Syntax Tree (AST) and identifying key elements like functions, classes, variables, and their relationships.\n\n\nProject-Dependent Relation Filtering:  RepoGraph intelligently filters out irrelevant dependencies, focusing only on the project-specific relationships crucial for understanding your codebase.\n\n\nGraph Organization: Finally, RepoGraph organizes the parsed information into a comprehensive graph structure, where nodes represent code lines and edges represent dependencies.  This graph becomes the foundation for powerful analysis and interaction.\n\n\nRepoGraph in Action: Integration and Evaluation\nThe researchers integrated RepoGraph into four different AI software engineering frameworks (two agent-based and two procedural) and evaluated its performance on two benchmarks: SWE-bench (for repository-level tasks) and CrossCodeEval (for general coding tasks).\nThe results were impressive.  RepoGraph led to a significant average relative improvement of 32.8% in success rate on SWE-bench.  CrossCodeEval results also showed substantial improvements in code and identifier matching.  The best results were achieved by using LLMs to summarize information extracted from the graph.\nBeyond the Benchmarks: Real-World Applications\nRepoGraph's potential extends far beyond the benchmarks.  Imagine its applications in:\n\nEnhanced Code Completion:  Suggesting relevant code snippets based on the broader context of the project.\nImproved Bug Detection: Identifying potential bugs by analyzing the interconnectedness of code components.\nMore Effective Code Refactoring:  Making informed changes that minimize the risk of introducing new problems.\nStreamlined Code Reviews: Providing a clearer, more concise understanding of code changes for reviewers.\n\nLimitations and Future Directions\nWhile RepoGraph shows immense promise, some limitations exist:\n\nBenchmark limitations: The current benchmarks might not fully represent the complexity of real-world projects.\nLLM dependence: RepoGraph's effectiveness relies on the quality of underlying LLMs.\nScalability: Further research is needed to ensure scalability for extremely large repositories.\nLanguage support: Currently focused on Python; expansion to other languages is a key goal.\n\nFuture research will focus on addressing these limitations and exploring new applications for RepoGraph.\nConclusion: The Future of AI Software Engineering is Graph-Based\nRepoGraph represents a significant leap forward in AI software engineering. By providing a repository-level understanding of code, RepoGraph empowers developers and AI tools to tackle more complex tasks more effectively.  This is not just an incremental improvementit's a paradigm shift.  The future of AI software engineering is graph-based, and RepoGraph is leading the way.\n","id":"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/","title":"RepoGraph: Enhancing AI Software Engineering with Repository-level Code Graph"},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"body":"Rethinking VLMs and LLMs for Image Classification\nThis post dives into a recent research paper that challenges conventional wisdom about Visual Language Models (VLMs) and Large Language Models (LLMs) in image classification.  Get ready to rethink your approach to combining these powerful tools!\nKey Findings: When LLMs Help (and When They Don't)\nThe research paper, \"Rethinking VLMs and LLMs for Image Classification,\" explores the effectiveness of combining VLMs and LLMs for image classification tasks.  The researchers ran extensive experiments across seven models, ten datasets, and numerous prompt variations.  Their results revealed some surprising findings:\n\n\nVLMs often beat VLM+LLM combos for basic tasks: Contrary to popular belief, VLMs without LLMs often outperformed VLM+LLM combinations when it came to basic object and scene recognition.  This suggests that adding an LLM isn't always a performance boost for simpler image classification tasks.\n\n\nLLMs shine with reasoning and external knowledge: However, the story changes when the task involves reasoning and requires external knowledge.  In these more complex scenarios, VLM+LLMs demonstrated a significant performance advantage over VLMs alone.\n\n\nThe LLM Router: A Cost-Effective Solution\nTo harness the strengths of both VLMs and LLMs efficiently, the researchers developed a lightweight \"LLM router.\" This small LLM acts as a task manager, intelligently deciding which model (VLM or VLM+LLM) is best suited for a given image classification task.  The results are impressive:\n\n\nState-of-the-art accuracy: The LLM router matched or surpassed the performance of heavyweight methods like GPT-4V and HuggingGPT.\n\n\nImproved cost-effectiveness:  This smart routing approach significantly reduces the computational cost compared to running multiple models for every task.\n\n\nBeyond the Benchmarks:  Future Directions\nWhile the LLM router shows promising results, the researchers also identified some areas for future exploration:\n\n\nGeneralizability: More research is needed to confirm the router's effectiveness across a wider range of tasks and datasets.\n\n\nData quality matters: The router's performance is directly linked to the quality of its training data.  Addressing biases and inaccuracies in the training data is crucial.\n\n\nSophisticated routing strategies:  Exploring more advanced routing methods, like ensemble approaches or hierarchical routing, could further enhance performance.\n\n\nMultimodal inputs: Incorporating additional data types, beyond text and image metadata, could improve the router's decision-making capabilities.\n\n\nConclusion:  A Smarter Approach to VLM+LLM Integration\nThis research highlights that a more strategic approach to integrating VLMs and LLMs is needed. Instead of always combining them directly, a system that intelligently chooses the best model for the task at hand can yield superior results with improved efficiency. The LLM router offers a compelling example of this smarter approach, promising exciting possibilities for the future of image classification and beyond.\n","id":"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/","title":"Rethinking VLMs and LLMs for Image Classification"},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"body":"Boosting Targeted Attacks: The SST Transformation\nIntroduction: The Challenge of Transferable Targeted Attacks\nTransferable targeted adversarial attacks (TTAs) against deep neural networks (DNNs) are significantly more challenging than untargeted attacks.  Current solutions either demand extensive extra data and training or severely overfit to the surrogate model. This paper introduces a novel transformation method, Strong, Self-transferable, fast, and Simple Scale Transformation (SST), designed to overcome these limitations and significantly improve TTAs.  Instead of focusing solely on loss functions, SST emphasizes the often-overlooked role of image transformations in gradient calculations.  Let's dive in!\nThe SST Approach: A Novel Transformation Strategy\nSST is built on several key insights:\n\nImage transformations are crucial: Simple TTAs struggle with the gradient vanishing problem.  Image transformations are key to mitigating this and enhancing transferability.\nSelf-transferability predicts black-box transferability:  How well adversarial perturbations transfer to different transformations of the same image (self-transferability) is a strong indicator of their black-box transferability.\nSimple scaling is surprisingly effective:  A scaling-centered strategy proves exceptionally effective at improving targeted transferability, outperforming many complex transformations.\n\nUsing these insights, SST incorporates:\n\nModified scaling transformations\nComplementary transformations\nBlock-wise scaling\n\nThis multifaceted approach results in a highly efficient and effective TTA method.\nKey Findings and Results\nExtensive experiments on the ImageNet-Compatible dataset demonstrated SST's superiority:\n\nState-of-the-art (SOTA) performance: SST achieved SOTA average targeted transfer success rates, outperforming the previous leading method by over 14%.\nSignificant efficiency gains: SST required only 25.7% of the execution time compared to the previous SOTA method.\nReal-world effectiveness:  SST demonstrated effectiveness against real-world APIs.\n\nThese results validate the hypotheses and demonstrate SST's significant advancement in the field.\nLimitations and Future Directions\nWhile SST shows great promise, it's important to acknowledge some limitations:\n\nThe study used a diverse but not exhaustive set of black-box models.  Further research should explore generalizability to other DNN architectures and defense mechanisms.\nThe findings are based on the ImageNet-Compatible dataset.  Generalizability to other datasets requires further investigation.\nSST's performance depends on the choice of surrogate model. More research is needed to assess the impact of different surrogate models.\n\nFuture research could explore:\n\nOther types of image transformations.\nMore robust blind estimators for black-box transferability prediction.\nApplications of SST to other security-critical domains (autonomous driving, medical image analysis, etc.).\nDevelopment of robust defense mechanisms against TTAs, informed by the insights gained from this research.\n\nConclusion: SST  A Powerful New Tool for TTA Research\nSST represents a significant leap forward in transferable targeted adversarial attacks. It provides a highly efficient and effective method for generating transferable targeted adversarial examples, surpassing existing methods in both effectiveness and efficiency. The research underscores the critical role of image transformations and offers valuable insights for designing and evaluating future transformation strategies. This work highlights the realistic threats posed by TTAs and paves the way for more robust defense mechanisms.\n","id":"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/","title":"S4ST: A Strong, Self-transferable, faSt, and Simple Scale Transformation for Transferable Targeted Attack"},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"body":"Conceptors for Steering LLMs: Better Than Additive Vectors?\nIntroduction: Fine-Tuning LLMs with Conceptors\nLarge language models (LLMs) are transforming AI, but controlling their output remains a challenge.  While methods like RLHF and fine-tuning exist, they're computationally expensive and may not generalize well.  Prompt engineering, while simpler, often yields inconsistent results.  Activation engineering offers a promising alternative: directly tweaking the model's activations during inference to steer its output.  But traditional methods, relying on single \"steering vectors,\" often fall short, especially on complex tasks.\nThis blog post dives into a research paper, \"Steering Large Language Models using Conceptors: Improving Addition-Based Activation Engineering,\" which introduces a novel approach: using conceptors to achieve more precise LLM control.  Conceptors are mathematical constructs representing sets of activation vectors as ellipsoidal regions.  Think of them as sophisticated, soft projection matrices offering far more nuanced control than simple steering vectors.\nWhy Conceptors Outshine Simple Steering Vectors\nThe core idea is simple yet powerful: instead of a single vector, conceptors capture the distribution of activation patterns associated with a specific task. This richer representation allows for more robust and accurate steering.  The authors hypothesize, and demonstrate, that:\n\nHypothesis 1: Conceptors provide more accurate LLM control than additive steering vectors.\nHypothesis 2: Combining multiple steering goals using Boolean operations (specifically AND) on conceptors outperforms simply averaging the corresponding steering vectors.\n\nMethodology: Putting Conceptors to the Test\nThe researchers tested their conceptor approach against traditional additive steering vectors using two LLMs: GPT-J 6B and GPT-NeoX 20B. They used a dataset of input-output function examples (antonyms, tense changes, translation, singular-plural, country-capital, capitalization) from prior work.\nFor each function, they:\n\nComputed both additive steering vectors and conceptor matrices from in-context learning prompts.\nUsed these to steer model activations during inference.\nEvaluated performance on unseen examples using accuracy as the metric.\nOptimized hyperparameters (aperture for conceptors, scaling coefficients for both methods) via grid search.\nExplored mean-centering to improve additive steering.\n\nResults: Conceptors Reign Supreme\nThe results were clear and consistent: conceptor-based steering significantly outperformed additive steering across all functions and LLMs.  This advantage was especially pronounced for complex tasks.  Combining conceptors using Boolean operations also trumped averaging steering vectors for multiple goals.  While mean-centering boosted additive steering, conceptors still maintained a clear lead.  The optimal layers for steering were also identified for both models and tasks.\nLimitations and Future Directions\nWhile promising, the study acknowledges limitations:\n\nComputational Cost: Conceptors are more computationally expensive than additive vectors.\nData Dependency:  Their effectiveness relies on sufficient training data.\nHyperparameter Tuning:  The aperture parameter requires careful tuning.\n\nFuture research should address these limitations and explore:\n\nScalability to larger LLMs.\nSteering more complex behaviors.\nOther Boolean operations on conceptors.\nCombining conceptors with other LLM steering techniques.\nUtilizing conceptors for bias mitigation.\n\nConclusion: A New Era in LLM Steering?\nThis research strongly suggests that conceptor-based steering offers a significant improvement over traditional additive methods for controlling LLMs. The ability to capture and manipulate the distribution of activation patterns opens up exciting possibilities for more precise and robust LLM control, paving the way for more reliable and safer AI systems.  The increased computational cost is a hurdle, but the potential benefits warrant further investigation.\n","id":"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/","title":"Steering Large Language Models using Conceptors: Improving Addition-Based Activation Engineering"},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"body":"Transformers: Efficient Compilers, Provably\nIntroduction\nLarge language models (LLMs) based on transformers have shown impressive results in various language tasks, including programming. This research paper delves into the potential of using transformers as compilers, a novel application that could significantly impact software development. The authors present a formal analysis comparing the efficiency of transformers to recurrent neural networks (RNNs) for compiler tasks.  They introduce a new C-like language, Mini-Husky, and a domain-specific language, Cybertron, to facilitate their research.\nMini-Husky: A C-like Language for Compiler Experiments\nTo conduct their experiments in a controlled environment, the researchers developed Mini-Husky, a C-like programming language. This language includes key features of modern C-like languages, such as structs, enums, and strict type checking, making it well-suited for testing the capabilities of transformers as compilers.  The choice of Mini-Husky allows for a focused investigation of the transformer's ability to handle core compiler tasks.\nCompiler Tasks: AST Construction, Symbol Resolution, and Type Analysis\nThe study focuses on three core compiler tasks:\n\nAbstract Syntax Tree (AST) Construction: Transforming code into its tree-like representation (the AST).  This is a crucial initial step in compilation.\nSymbol Resolution: Verifying that all references to variables and functions are valid and correctly defined.\nType Analysis: Checking the type correctness of the code, including type inference and type checking. This helps catch errors before runtime.\n\nLogarithmic Parameter Scaling: Transformers' Efficiency Advantage\nA key finding of the paper is that, under certain assumptions (bounded depth in the Abstract Syntax Tree (AST) and type inference), the number of parameters required by transformers to perform these compiler tasks scales logarithmically with the input sequence length. This is a significant improvement over RNNs, which exhibit linear parameter scaling. This logarithmic scaling implies that transformers are exponentially more efficient than RNNs for handling long code sequences, particularly relevant for large-scale projects.\nCybertron: A DSL for Formal Analysis\nTo formally prove their theoretical results about the efficiency of transformers, the researchers developed Cybertron, a domain-specific language.  Cybertron enables formal reasoning about the capabilities of transformer architectures in a way that is more straightforward than using traditional natural language proofs. This DSL assists in proving the logarithmic parameter scaling for transformers, contributing a novel approach to analyzing neural network architectures.\nEmpirical Validation: Transformers Outperform RNNs\nThe research includes empirical validation using Mini-Husky.  Experiments comparing transformers and RNNs on the three compiler tasks showed that transformers consistently outperform RNNs, especially as the input size increases. These results support the theoretical findings, demonstrating the practical advantage of transformers for compilation.\nLimitations and Future Work\nWhile the research presents compelling results, it also acknowledges certain limitations:\n\nBounded Depth Assumption: The theoretical analysis relies on the assumption of bounded AST and type inference depth.  This might not always hold true for complex real-world programs.\nSynthetic Data: The empirical validation used synthetic data.  Further testing with real-world codebases is needed to fully assess the performance of these models.\nCybertron's General Applicability: The generalizability of Cybertron to other neural network architectures requires further exploration.\n\nFuture research could focus on relaxing the bounded depth assumption, evaluating the models on real-world code, and extending Cybertron's capabilities.  Exploring different transformer architectures and applying these findings to other software engineering domains are also promising avenues for future work.\nConclusion\nThe paper provides strong evidence that transformers offer a significant efficiency advantage over RNNs for compilation tasks, particularly for well-structured code.  The development of Cybertron highlights a promising new method for formally analyzing the capabilities of neural network architectures.  The findings could have a major impact on the future of compiler design and software development.\n","id":"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/","title":"Transformers are Efficient Compilers, Provably"},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"body":"Efficient Neural CBF Verification\nIntroduction\nEnsuring safety in control systems, especially those involving robots, is paramount. Control Barrier Functions (CBFs) have emerged as a powerful tool for guaranteeing safety constraints.  Recently, neural networks have been used to parameterize CBFs, offering the advantage of handling complex systems. However, verifying the correctness of these neural CBFs is a significant challenge.  This post dives into a groundbreaking research paper that tackles this problem head-on, presenting a novel verification framework that's significantly faster and more reliable than existing methods.\nThe Challenge of Neural CBF Verification\nTraditional methods for verifying CBFs often struggle with the complexity of neural networks.  Their non-linearity and lack of explicit mathematical interpretability make it difficult to definitively prove that a neural CBF will always maintain safety constraints across all possible system states and inputs. Existing techniques, such as interval bound propagation, often lead to overly conservative bounds, resulting in many false negatives (failing to verify a correct CBF).\nA Novel Approach: Symbolic Derivative Bounds Propagation\nThe research paper, \"Verification of Neural Control Barrier Functions with Symbolic Derivative Bounds Propagation,\" introduces a novel solution.  Instead of relying on interval arithmetic, it leverages symbolic derivative bound propagation. This clever technique combines:\n\nLinear Symbolic Bounds for Dynamics Propagation:  This efficiently simplifies the system dynamics, avoiding the computationally expensive task of checking all possible input combinations.\nBounding the Jacobian of the Neural CBF:  This step utilizes the piecewise linear nature of ReLU activation functions to derive tighter bounds on the neural CBF's gradient.\nVerification with Symbolic Bound Propagation:  Finally, it combines the bounds from steps 1 and 2 to formulate the verification problem as a linear constraint satisfaction problem. A branch-and-bound technique is then used to efficiently search for solutions.\n\nThis approach leads to significantly tighter bounds compared to existing methods, resulting in higher verification rates and faster computation times.\nExperimental Results: A Clear Win\nThe researchers tested their method on various robot dynamics (Dubins car, point robot, planar quadrotor) with varying neural network complexities.  The results are impressive: their method achieved a 20% higher verified rate with reduced verification time compared to state-of-the-art baselines (NNCB-IBP and BBV).  Heatmaps and tables vividly illustrate the superiority of this new approach.\nLimitations and Future Directions\nWhile impressive, the method does have limitations.  Scalability remains a concern, as computational cost can still increase significantly with higher-dimensional state spaces and more complex networks.  Additionally, the reliance on ReLU activation functions and approximations in the system dynamics and optimal control inputs could lead to false negatives or less tight bounds.  Future research could focus on extending this method to other activation functions, refining approximation techniques, and integrating verification directly into the neural CBF training process.\nConclusion: A Step Towards Safer Autonomous Systems\nThis research presents a significant advance in verifying neural CBFs. The proposed symbolic derivative bound propagation method offers a substantial improvement in efficiency and effectiveness, paving the way for more reliable and safer autonomous systems.  The higher verification rates and faster computation times are compelling evidence of its potential to transform safety-critical control system design.\n","id":"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/","title":"Verification of Neural Control Barrier Functions with Symbolic Derivative Bounds Propagation"}},"docInfo":{"http://127.0.0.1:1111/":{"body":0,"title":0},"http://127.0.0.1:1111/blog/":{"body":0,"title":2},"http://127.0.0.1:1111/blog/articulate-anything-automatic-modeling/":{"body":401,"title":11},"http://127.0.0.1:1111/blog/boosting-llm-translation-skills/":{"body":389,"title":11},"http://127.0.0.1:1111/blog/controlled-low-rank-adaptation-llms/":{"body":382,"title":11},"http://127.0.0.1:1111/blog/cool-program-synthesis-framework/":{"body":523,"title":13},"http://127.0.0.1:1111/blog/deep-domain-isolation-scfl-for-segmentation/":{"body":410,"title":9},"http://127.0.0.1:1111/blog/interpreting-llm-features-with-sae/":{"body":446,"title":7},"http://127.0.0.1:1111/blog/llm-invented-algorithms-self-improve/":{"body":281,"title":7},"http://127.0.0.1:1111/blog/llmscan-causal-scan-llm-misbehavior/":{"body":400,"title":6},"http://127.0.0.1:1111/blog/quailora-lora-quantization/":{"body":388,"title":5},"http://127.0.0.1:1111/blog/repograph-ai-software-engineering/":{"body":427,"title":9},"http://127.0.0.1:1111/blog/rethinking-vlms-and-llms-for-image-classification/":{"body":296,"title":5},"http://127.0.0.1:1111/blog/sst-strong-self-transferable-attack/":{"body":356,"title":11},"http://127.0.0.1:1111/blog/steering-llms-using-conceptors/":{"body":401,"title":11},"http://127.0.0.1:1111/blog/transformers-are-efficient-compilers/":{"body":421,"title":4},"http://127.0.0.1:1111/blog/verify-neural-cbf-with-bounds/":{"body":362,"title":9}},"length":17},"lang":"English"}